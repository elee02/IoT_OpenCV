{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Preprocessing image for face detection\n",
      "[INFO] Running inference on the image\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "session = ort.InferenceSession(\"../data/models/yolov8n.onnx\")\n",
    "input_name = session.get_inputs()[0].name\n",
    "\n",
    "image = cv2.imread(\"../data/test_images/common.jpg\")\n",
    "\n",
    "# Preprocess image for ONNX model\n",
    "print(\"[INFO] Preprocessing image for face detection\")\n",
    "# Resize image to the required size for YOLO model\n",
    "input_image = cv2.resize(image, (640, 640))\n",
    "# Normalize pixel values to range [0, 1]\n",
    "input_image = input_image.astype(np.float32) / 255.0\n",
    "# Change image layout to channel-first format as required by ONNX model\n",
    "input_image = np.transpose(input_image, (2, 0, 1))  # Channel first\n",
    "# Add batch dimension (needed by the model)\n",
    "input_image = np.expand_dims(input_image, axis=0)\n",
    "\n",
    "# Perform inference on the provided image\n",
    "print(\"[INFO] Running inference on the image\")\n",
    "\n",
    "output = session.run(\n",
    "    output_names=None, \n",
    "    input_feed= {input_name: input_image}\n",
    ")\n",
    "outputs = output[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 22.25124 ,  63.748386,  87.80733 , 129.69324 , 174.27881 ,\n",
       "       192.77115 , 213.32562 , 269.3759  , 316.48227 , 322.21863 ,\n",
       "       320.0763  , 316.99564 , 318.22125 , 326.87085 , 433.69553 ,\n",
       "       461.33926 , 492.1537  , 521.9314  , 574.0167  , 586.5244  ,\n",
       "        22.237974,  64.08118 , 112.29887 , 154.78146 , 179.81332 ,\n",
       "       190.1046  , 224.40012 , 279.797   , 318.0047  , 318.1396  ,\n",
       "       320.30872 , 317.26102 , 318.1406  , 335.68372 , 430.21753 ,\n",
       "       459.9945  , 492.00806 , 498.9185  , 567.9201  , 587.4642  ,\n",
       "        22.73684 ,  68.41667 , 181.43848 , 173.88988 , 179.5802  ,\n",
       "       191.43561 , 238.93619 , 288.25153 , 318.90552 , 317.70673 ,\n",
       "       321.46643 , 318.6952  , 319.8886  , 353.45447 , 402.1404  ,\n",
       "       459.18643 , 494.7537  , 490.04715 , 542.8994  , 588.4896  ,\n",
       "        22.670723,  91.90434 , 216.26353 , 197.72499 , 181.47786 ,\n",
       "       196.88611 , 270.8847  , 307.2757  , 320.03256 , 318.68646 ,\n",
       "       323.23904 , 320.78094 , 317.70187 , 334.6641  , 363.4209  ,\n",
       "       436.91595 , 483.63205 , 472.1972  , 524.3074  , 590.328   ,\n",
       "        24.313564, 143.87476 , 197.07613 , 214.93752 , 202.09161 ,\n",
       "       218.15901 , 282.89044 , 311.0667  , 319.37445 , 317.96368 ,\n",
       "       324.19092 , 313.88373 , 314.94144 , 326.45734 , 349.84894 ,\n",
       "       391.12936 , 453.38788 , 472.55856 , 497.5859  , 574.4389  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0][8300:8400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces = []\n",
    "\n",
    "for detection in outputs:\n",
    "    # Extract bounding box coordinates\n",
    "    center_x, center_y, width, height = detection[:4]\n",
    "\n",
    "    # Denormalize to original image size (assuming image size of 640x640)\n",
    "    center_x *= 640\n",
    "    center_y *= 640\n",
    "    width *= 640\n",
    "    height *= 640\n",
    "\n",
    "    # Convert to (x1, y1, x2, y2)\n",
    "    x1 = int(center_x - width / 2)\n",
    "    y1 = int(center_y - height / 2)\n",
    "    x2 = int(center_x + width / 2)\n",
    "    y2 = int(center_y + height / 2)\n",
    "\n",
    "    # Extract objectness score\n",
    "    objectness_score = detection[4]\n",
    "    confidence_threshold = 0.65\n",
    "    if objectness_score < confidence_threshold:\n",
    "        continue\n",
    "\n",
    "    # Extract class scores and determine the class with highest confidence\n",
    "    class_scores = detection[5:]\n",
    "    class_id = np.argmax(class_scores)\n",
    "    class_confidence = class_scores[class_id]\n",
    "\n",
    "    # Set a class confidence threshold\n",
    "    if class_confidence > 0.5:\n",
    "        faces.append([x1, y1, x2, y2, class_id, class_confidence])\n",
    "        # Optionally draw the bounding box on the image\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), (255, 0, 0), 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_OpNamespace' 'torchvision' object has no attribute 'nms'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 57\u001b[0m\n\u001b[1;32m     55\u001b[0m boxes_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(boxes)\n\u001b[1;32m     56\u001b[0m confidences_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(confidences)\n\u001b[0;32m---> 57\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnms\u001b[49m(boxes_tensor, confidences_tensor, iou_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m     58\u001b[0m boxes \u001b[38;5;241m=\u001b[39m boxes[indices]\n\u001b[1;32m     59\u001b[0m confidences \u001b[38;5;241m=\u001b[39m confidences[indices]\n",
      "File \u001b[0;32m~/PiPresence/.venv/lib/python3.10/site-packages/torch/_ops.py:1225\u001b[0m, in \u001b[0;36m_OpNamespace.__getattr__\u001b[0;34m(self, op_name)\u001b[0m\n\u001b[1;32m   1223\u001b[0m     op, overload_names \u001b[38;5;241m=\u001b[39m _get_packet(qualified_op_name, module_name)\n\u001b[1;32m   1224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m op \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1225\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1226\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_OpNamespace\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mop_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1227\u001b[0m         )\n\u001b[1;32m   1228\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1229\u001b[0m     \u001b[38;5;66;03m# Turn this into AttributeError so getattr(obj, key, default)\u001b[39;00m\n\u001b[1;32m   1230\u001b[0m     \u001b[38;5;66;03m# works (this is called by TorchScript with __origin__)\u001b[39;00m\n\u001b[1;32m   1231\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1232\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_OpNamespace\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mop_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1233\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_OpNamespace' 'torchvision' object has no attribute 'nms'"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Load the model\n",
    "session = ort.InferenceSession(\"../data/models/yolov8n.onnx\")\n",
    "input_name = session.get_inputs()[0].name\n",
    "\n",
    "# Load and preprocess the image\n",
    "image = cv2.imread(\"../data/test_images/common.jpg\")\n",
    "input_image = cv2.resize(image, (640, 640))\n",
    "input_image = input_image.astype(np.float32) / 255.0\n",
    "input_image = np.transpose(input_image, (2, 0, 1))  # Channel first\n",
    "input_image = np.expand_dims(input_image, axis=0)\n",
    "\n",
    "# Run inference\n",
    "output = session.run(None, {input_name: input_image})\n",
    "\n",
    "# Process the outputs\n",
    "outputs = output[0]\n",
    "outputs = outputs.transpose(0, 2, 1)\n",
    "outputs = outputs[0]\n",
    "\n",
    "# Extract boxes and scores\n",
    "boxes = outputs[:, :4]\n",
    "scores = outputs[:, 4:]\n",
    "\n",
    "# Apply activation functions\n",
    "class_probs = 1 / (1 + np.exp(-scores))\n",
    "\n",
    "# Decode bounding boxes\n",
    "boxes[:, 0] = boxes[:, 0] - boxes[:, 2] / 2  # x1\n",
    "boxes[:, 1] = boxes[:, 1] - boxes[:, 3] / 2  # y1\n",
    "boxes[:, 2] = boxes[:, 0] + boxes[:, 2]      # x2\n",
    "boxes[:, 3] = boxes[:, 1] + boxes[:, 3]      # y2\n",
    "\n",
    "# Filter predictions\n",
    "confidences = np.max(class_probs, axis=1)\n",
    "class_ids = np.argmax(class_probs, axis=1)\n",
    "conf_threshold = 0.5\n",
    "mask = confidences > conf_threshold\n",
    "boxes = boxes[mask]\n",
    "confidences = confidences[mask]\n",
    "class_ids = class_ids[mask]\n",
    "\n",
    "# Rescale boxes\n",
    "orig_height, orig_width = image.shape[:2]\n",
    "scale_x = orig_width / 640\n",
    "scale_y = orig_height / 640\n",
    "boxes[:, [0, 2]] *= scale_x\n",
    "boxes[:, [1, 3]] *= scale_y\n",
    "\n",
    "# Apply NMS\n",
    "boxes_tensor = torch.tensor(boxes)\n",
    "confidences_tensor = torch.tensor(confidences)\n",
    "indices = torch.ops.torchvision.nms(boxes_tensor, confidences_tensor, iou_threshold=0.5)\n",
    "boxes = boxes[indices]\n",
    "confidences = confidences[indices]\n",
    "class_ids = class_ids[indices]\n",
    "\n",
    "# Draw boxes on the image\n",
    "for box, conf, class_id in zip(boxes, confidences, class_ids):\n",
    "    x1, y1, x2, y2 = box.astype(int)\n",
    "    label = f\"Class {class_id}: {conf:.2f}\"\n",
    "    cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "    cv2.putText(image, label, (x1, y1 - 10),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "# Display the image\n",
    "cv2.imshow(\"Detections\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ../data/models/yolov8n.onnx for ONNX Runtime inference...\n",
      "WARNING ⚠️ Failed to start ONNX Runtime session with CUDA. Falling back to CPU...\n",
      "Preferring ONNX Runtime AzureExecutionProvider\n",
      "\n",
      "image 1/1 /home/el02/PiPresence/tests/../data/known_faces/elyor/front.jpg: 640x640 1 person, 33.7ms\n",
      "Speed: 0.7ms preprocess, 33.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"../data/models/yolov8n.onnx\", task=\"detect\")\n",
    "results = model(\"../data/known_faces/elyor/front.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer names:\n",
      "['', 'model', 'model.0', 'model.0.conv', 'model.0.bn', 'model.0.act', 'model.1', 'model.1.conv', 'model.1.bn', 'model.2', 'model.2.cv1', 'model.2.cv1.conv', 'model.2.cv1.bn', 'model.2.cv2', 'model.2.cv2.conv', 'model.2.cv2.bn', 'model.2.m', 'model.2.m.0', 'model.2.m.0.cv1', 'model.2.m.0.cv1.conv', 'model.2.m.0.cv1.bn', 'model.2.m.0.cv2', 'model.2.m.0.cv2.conv', 'model.2.m.0.cv2.bn', 'model.3', 'model.3.conv', 'model.3.bn', 'model.4', 'model.4.cv1', 'model.4.cv1.conv', 'model.4.cv1.bn', 'model.4.cv2', 'model.4.cv2.conv', 'model.4.cv2.bn', 'model.4.m', 'model.4.m.0', 'model.4.m.0.cv1', 'model.4.m.0.cv1.conv', 'model.4.m.0.cv1.bn', 'model.4.m.0.cv2', 'model.4.m.0.cv2.conv', 'model.4.m.0.cv2.bn', 'model.4.m.1', 'model.4.m.1.cv1', 'model.4.m.1.cv1.conv', 'model.4.m.1.cv1.bn', 'model.4.m.1.cv2', 'model.4.m.1.cv2.conv', 'model.4.m.1.cv2.bn', 'model.5', 'model.5.conv', 'model.5.bn', 'model.6', 'model.6.cv1', 'model.6.cv1.conv', 'model.6.cv1.bn', 'model.6.cv2', 'model.6.cv2.conv', 'model.6.cv2.bn', 'model.6.m', 'model.6.m.0', 'model.6.m.0.cv1', 'model.6.m.0.cv1.conv', 'model.6.m.0.cv1.bn', 'model.6.m.0.cv2', 'model.6.m.0.cv2.conv', 'model.6.m.0.cv2.bn', 'model.6.m.1', 'model.6.m.1.cv1', 'model.6.m.1.cv1.conv', 'model.6.m.1.cv1.bn', 'model.6.m.1.cv2', 'model.6.m.1.cv2.conv', 'model.6.m.1.cv2.bn', 'model.7', 'model.7.conv', 'model.7.bn', 'model.8', 'model.8.cv1', 'model.8.cv1.conv', 'model.8.cv1.bn', 'model.8.cv2', 'model.8.cv2.conv', 'model.8.cv2.bn', 'model.8.m', 'model.8.m.0', 'model.8.m.0.cv1', 'model.8.m.0.cv1.conv', 'model.8.m.0.cv1.bn', 'model.8.m.0.cv2', 'model.8.m.0.cv2.conv', 'model.8.m.0.cv2.bn', 'model.9', 'model.9.cv1', 'model.9.cv1.conv', 'model.9.cv1.bn', 'model.9.cv2', 'model.9.cv2.conv', 'model.9.cv2.bn', 'model.9.m', 'model.10', 'model.11', 'model.12', 'model.12.cv1', 'model.12.cv1.conv', 'model.12.cv1.bn', 'model.12.cv2', 'model.12.cv2.conv', 'model.12.cv2.bn', 'model.12.m', 'model.12.m.0', 'model.12.m.0.cv1', 'model.12.m.0.cv1.conv', 'model.12.m.0.cv1.bn', 'model.12.m.0.cv2', 'model.12.m.0.cv2.conv', 'model.12.m.0.cv2.bn', 'model.13', 'model.14', 'model.15', 'model.15.cv1', 'model.15.cv1.conv', 'model.15.cv1.bn', 'model.15.cv2', 'model.15.cv2.conv', 'model.15.cv2.bn', 'model.15.m', 'model.15.m.0', 'model.15.m.0.cv1', 'model.15.m.0.cv1.conv', 'model.15.m.0.cv1.bn', 'model.15.m.0.cv2', 'model.15.m.0.cv2.conv', 'model.15.m.0.cv2.bn', 'model.16', 'model.16.conv', 'model.16.bn', 'model.17', 'model.18', 'model.18.cv1', 'model.18.cv1.conv', 'model.18.cv1.bn', 'model.18.cv2', 'model.18.cv2.conv', 'model.18.cv2.bn', 'model.18.m', 'model.18.m.0', 'model.18.m.0.cv1', 'model.18.m.0.cv1.conv', 'model.18.m.0.cv1.bn', 'model.18.m.0.cv2', 'model.18.m.0.cv2.conv', 'model.18.m.0.cv2.bn', 'model.19', 'model.19.conv', 'model.19.bn', 'model.20', 'model.21', 'model.21.cv1', 'model.21.cv1.conv', 'model.21.cv1.bn', 'model.21.cv2', 'model.21.cv2.conv', 'model.21.cv2.bn', 'model.21.m', 'model.21.m.0', 'model.21.m.0.cv1', 'model.21.m.0.cv1.conv', 'model.21.m.0.cv1.bn', 'model.21.m.0.cv2', 'model.21.m.0.cv2.conv', 'model.21.m.0.cv2.bn', 'model.22', 'model.22.cv2', 'model.22.cv2.0', 'model.22.cv2.0.0', 'model.22.cv2.0.0.conv', 'model.22.cv2.0.0.bn', 'model.22.cv2.0.1', 'model.22.cv2.0.1.conv', 'model.22.cv2.0.1.bn', 'model.22.cv2.0.2', 'model.22.cv2.1', 'model.22.cv2.1.0', 'model.22.cv2.1.0.conv', 'model.22.cv2.1.0.bn', 'model.22.cv2.1.1', 'model.22.cv2.1.1.conv', 'model.22.cv2.1.1.bn', 'model.22.cv2.1.2', 'model.22.cv2.2', 'model.22.cv2.2.0', 'model.22.cv2.2.0.conv', 'model.22.cv2.2.0.bn', 'model.22.cv2.2.1', 'model.22.cv2.2.1.conv', 'model.22.cv2.2.1.bn', 'model.22.cv2.2.2', 'model.22.cv3', 'model.22.cv3.0', 'model.22.cv3.0.0', 'model.22.cv3.0.0.conv', 'model.22.cv3.0.0.bn', 'model.22.cv3.0.1', 'model.22.cv3.0.1.conv', 'model.22.cv3.0.1.bn', 'model.22.cv3.0.2', 'model.22.cv3.1', 'model.22.cv3.1.0', 'model.22.cv3.1.0.conv', 'model.22.cv3.1.0.bn', 'model.22.cv3.1.1', 'model.22.cv3.1.1.conv', 'model.22.cv3.1.1.bn', 'model.22.cv3.1.2', 'model.22.cv3.2', 'model.22.cv3.2.0', 'model.22.cv3.2.0.conv', 'model.22.cv3.2.0.bn', 'model.22.cv3.2.1', 'model.22.cv3.2.1.conv', 'model.22.cv3.2.1.bn', 'model.22.cv3.2.2', 'model.22.dfl', 'model.22.dfl.conv']\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the model\n",
    "model = YOLO('../data/models/yolov8n.pt')\n",
    "\n",
    "# Get the model architecture (Pytorch model)\n",
    "yolo_model = model.model\n",
    "\n",
    "# Get the names of all layers\n",
    "layer_names = [name for name, _ in yolo_model.named_modules()]\n",
    "\n",
    "print(\"Layer names:\")\n",
    "print(layer_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/el02/PiPresence/.venv/lib/python3.10/site-packages/ultralytics/nn/modules/head.py:105: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if self.format != \"imx\" and (self.dynamic or self.shape != shape):\n",
      "/home/el02/PiPresence/.venv/lib/python3.10/site-packages/ultralytics/utils/tal.py:308: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n",
      "  for i, stride in enumerate(strides):\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define a dummy input\n",
    "dummy_input = torch.randn(1, 3, 640, 640)\n",
    "\n",
    "# Export to ONNX\n",
    "torch.onnx.export(yolo_model, dummy_input, \"data/models/yolov8n.onnx\",\n",
    "                  input_names=['input'],\n",
    "                  output_names=['output_boxes', 'output_classes'],\n",
    "                  dynamic_axes={'input': {0: 'batch_size'}, 'output_boxes': {0: 'batch_size'}, 'output_classes': {0: 'batch_size'}})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Preprocessing image for face detection\n",
      "[INFO] Running inference on the image\n",
      "Output shape: (1, 5, 8400)\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the ONNX model\n",
    "session = ort.InferenceSession(\"../data/models/yolov8n-face.onnx\")\n",
    "\n",
    "# Get the input name for ONNX model\n",
    "input_name = session.get_inputs()[0].name\n",
    "\n",
    "# Get the output names from ONNX model\n",
    "output_names = [output.name for output in session.get_outputs()]\n",
    "\n",
    "# Load and preprocess the image\n",
    "image = cv2.imread(\"../data/test_images/friends_gathering.jpg\")\n",
    "print(\"[INFO] Preprocessing image for face detection\")\n",
    "input_image = cv2.resize(image, (640, 640))\n",
    "input_image = input_image.astype(np.float32) / 255.0\n",
    "input_image = np.transpose(input_image, (2, 0, 1))  # Channel first\n",
    "input_image = np.expand_dims(input_image, axis=0)\n",
    "\n",
    "# Perform inference on the provided image\n",
    "print(\"[INFO] Running inference on the image\")\n",
    "outputs = session.run(output_names, {input_name: input_image})\n",
    "\n",
    "# Output shape is likely (1, 84, 8400)\n",
    "print(\"Output shape:\", outputs[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ultralytics YOLO 🚀, AGPL-3.0 license\n",
    "\n",
    "import argparse\n",
    "\n",
    "import cv2.dnn\n",
    "import numpy as np\n",
    "\n",
    "from ultralytics.utils import ASSETS, yaml_load\n",
    "from ultralytics.utils.checks import check_yaml\n",
    "\n",
    "CLASSES = yaml_load(check_yaml(\"coco8.yaml\"))[\"names\"]\n",
    "colors = np.random.uniform(0, 255, size=(len(CLASSES), 3))\n",
    "\n",
    "\n",
    "def draw_bounding_box(img, class_id, confidence, x, y, x_plus_w, y_plus_h):\n",
    "    \"\"\"\n",
    "    Draws bounding boxes on the input image based on the provided arguments.\n",
    "\n",
    "    Args:\n",
    "        img (numpy.ndarray): The input image to draw the bounding box on.\n",
    "        class_id (int): Class ID of the detected object.\n",
    "        confidence (float): Confidence score of the detected object.\n",
    "        x (int): X-coordinate of the top-left corner of the bounding box.\n",
    "        y (int): Y-coordinate of the top-left corner of the bounding box.\n",
    "        x_plus_w (int): X-coordinate of the bottom-right corner of the bounding box.\n",
    "        y_plus_h (int): Y-coordinate of the bottom-right corner of the bounding box.\n",
    "    \"\"\"\n",
    "    label = f\"{CLASSES[class_id]} ({confidence:.2f})\"\n",
    "    color = colors[class_id]\n",
    "    cv2.rectangle(img, (x, y), (x_plus_w, y_plus_h), color, 2)\n",
    "    cv2.putText(img, label, (x - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "\n",
    "def main(onnx_model, input_image):\n",
    "    \"\"\"\n",
    "    Main function to load ONNX model, perform inference, draw bounding boxes, and display the output image.\n",
    "\n",
    "    Args:\n",
    "        onnx_model (str): Path to the ONNX model.\n",
    "        input_image (str): Path to the input image.\n",
    "\n",
    "    Returns:\n",
    "        list: List of dictionaries containing detection information such as class_id, class_name, confidence, etc.\n",
    "    \"\"\"\n",
    "    # Load the ONNX model\n",
    "    model: cv2.dnn.Net = cv2.dnn.readNetFromONNX(onnx_model)\n",
    "\n",
    "    # Read the input image\n",
    "    original_image: np.ndarray = cv2.imread(input_image)\n",
    "    [height, width, _] = original_image.shape\n",
    "\n",
    "    # Prepare a square image for inference\n",
    "    length = max((height, width))\n",
    "    image = np.zeros((length, length, 3), np.uint8)\n",
    "    image[0:height, 0:width] = original_image\n",
    "\n",
    "    # Calculate scale factor\n",
    "    scale = length / 640\n",
    "\n",
    "    # Preprocess the image and prepare blob for model\n",
    "    blob = cv2.dnn.blobFromImage(image, scalefactor=1 / 255, size=(640, 640), swapRB=True)\n",
    "    model.setInput(blob)\n",
    "\n",
    "    # Perform inference\n",
    "    outputs = model.forward()\n",
    "\n",
    "    # Prepare output array\n",
    "    outputs = np.array([cv2.transpose(outputs[0])])\n",
    "    rows = outputs.shape[1]\n",
    "\n",
    "    boxes = []\n",
    "    scores = []\n",
    "    class_ids = []\n",
    "\n",
    "    # Iterate through output to collect bounding boxes, confidence scores, and class IDs\n",
    "    for i in range(rows):\n",
    "        classes_scores = outputs[0][i][4:]\n",
    "        (minScore, maxScore, minClassLoc, (x, maxClassIndex)) = cv2.minMaxLoc(classes_scores)\n",
    "        if maxScore >= 0.25:\n",
    "            box = [\n",
    "                outputs[0][i][0] - (0.5 * outputs[0][i][2]),\n",
    "                outputs[0][i][1] - (0.5 * outputs[0][i][3]),\n",
    "                outputs[0][i][2],\n",
    "                outputs[0][i][3],\n",
    "            ]\n",
    "            boxes.append(box)\n",
    "            scores.append(maxScore)\n",
    "            class_ids.append(maxClassIndex)\n",
    "\n",
    "    # Apply NMS (Non-maximum suppression)\n",
    "    result_boxes = cv2.dnn.NMSBoxes(boxes, scores, 0.25, 0.45, 0.5)\n",
    "\n",
    "    detections = []\n",
    "\n",
    "    # Iterate through NMS results to draw bounding boxes and labels\n",
    "    for i in range(len(result_boxes)):\n",
    "        index = result_boxes[i]\n",
    "        box = boxes[index]\n",
    "        detection = {\n",
    "            \"class_id\": class_ids[index],\n",
    "            \"class_name\": CLASSES[class_ids[index]],\n",
    "            \"confidence\": scores[index],\n",
    "            \"box\": box,\n",
    "            \"scale\": scale,\n",
    "        }\n",
    "        detections.append(detection)\n",
    "        draw_bounding_box(\n",
    "            original_image,\n",
    "            class_ids[index],\n",
    "            scores[index],\n",
    "            round(box[0] * scale),\n",
    "            round(box[1] * scale),\n",
    "            round((box[0] + box[2]) * scale),\n",
    "            round((box[1] + box[3]) * scale),\n",
    "        )\n",
    "\n",
    "    # Display the image with bounding boxes\n",
    "    cv2.imshow(\"image\", original_image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'class_id': 0,\n",
       "  'class_name': 'person',\n",
       "  'confidence': 0.9291252493858337,\n",
       "  'box': [np.float32(72.9223),\n",
       "   np.float32(2.3677368),\n",
       "   np.float32(496.26514),\n",
       "   np.float32(353.2938)],\n",
       "  'scale': 1.875}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main(\"../data/models/yolov8n.onnx\", \"../data/test_images/common.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading YOLOv8n face-model from ../data/models/yolov8n-face.onnx\n"
     ]
    }
   ],
   "source": [
    "from pipresence.detect_faces import FaceDetector\n",
    "import cv2 \n",
    "\n",
    "detector = FaceDetector(\"../data/models/yolov8n-face.onnx\")\n",
    "image = cv2.imread(\"../data/known_faces/elyor/right.jpg\")\n",
    "detections = detector.detect_faces(image)\n",
    "detection = detections[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'class_id': 0, 'class_name': 'person', 'confidence': 0.3326736390590668, 'box': [np.float32(195.62439), np.float32(135.87161), np.float32(293.73694), np.float32(217.464)], 'scale': 1.0}]\n",
      "[INFO] 'q' pressed, exiting the application\n"
     ]
    }
   ],
   "source": [
    "print(detections)\n",
    "bbox = detection[\"box\"]\n",
    "x = round(bbox[0] * detection[\"scale\"])\n",
    "y = round(bbox[1] * detection[\"scale\"])\n",
    "x_plus_w = round((bbox[0] + bbox[2]) * detection[\"scale\"])\n",
    "y_plus_h = round((bbox[1] + bbox[3]) * detection[\"scale\"])\n",
    "color = (200, 56, 159)\n",
    "cv2.rectangle(image, (x, y), (x_plus_w, y_plus_h), color, 2)\n",
    "cv2.putText(image, \"Common\", (x - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "while True:\n",
    "    cv2.imshow(\"Example\", image)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        print(\"[INFO] 'q' pressed, exiting the application\")\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading MobileFaceNet model from ../data/models/mobilefacenet_fixed.onnx\n"
     ]
    },
    {
     "ename": "InvalidArgument",
     "evalue": "[ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Got invalid dimensions for input: input:0 for the following indices\n index: 1 Got: 3 Expected: 112\n index: 2 Got: 640 Expected: 112\n index: 3 Got: 640 Expected: 3\n Please fix either the inputs/outputs or the model.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgument\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m preprocessed_image \u001b[38;5;241m=\u001b[39m recognizer\u001b[38;5;241m.\u001b[39mpreprocess(image)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Run inference\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mrecognizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mrecognizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocessed_image\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PiPresence/.venv/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:266\u001b[0m, in \u001b[0;36mSession.run\u001b[0;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[1;32m    264\u001b[0m     output_names \u001b[38;5;241m=\u001b[39m [output\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs_meta]\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_feed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m C\u001b[38;5;241m.\u001b[39mEPFail \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_fallback:\n",
      "\u001b[0;31mInvalidArgument\u001b[0m: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Got invalid dimensions for input: input:0 for the following indices\n index: 1 Got: 3 Expected: 112\n index: 2 Got: 640 Expected: 112\n index: 3 Got: 640 Expected: 3\n Please fix either the inputs/outputs or the model."
     ]
    }
   ],
   "source": [
    "from pipresence.recognize_faces import FaceRecognizer\n",
    "from pipresence.config import Config \n",
    "import cv2\n",
    "\n",
    "Config.update_config(mobilefacenet_model_path=\"../data/models/mobilefacenet_fixed.onnx\")\n",
    "recognizer = FaceRecognizer()\n",
    "image = cv2.imread(\"../data/known_faces/tom/front.jpg\")\n",
    "preprocessed_image = recognizer.preprocess(image)\n",
    "\n",
    "# Run inference\n",
    "outputs = recognizer.session.run(None, {recognizer.input_name: preprocessed_image})\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading MobileFaceNet model from ../data/models/mobilefacenet0.onnx\n",
      "[ERROR] Face recognition failed: 'dict' object has no attribute 'shape'\n"
     ]
    }
   ],
   "source": [
    "from pipresence.recognize_faces import FaceRecognizer\n",
    "import cv2\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "recognizer = FaceRecognizer(\"../data/models/mobilefacenet0.onnx\")\n",
    "embeddings_file = \"../data/encodings/face_embeddings.pkl\"\n",
    "image = cv2.imread(\"../data/known_faces/elyor/right.jpg\")\n",
    "embedding = recognizer.recognize_face(detections[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading known face embeddings from ../data/encodings/face_embeddings.pkl\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "if os.path.exists(embeddings_file):\n",
    "    # Load existing embeddings from the file\n",
    "    print(f\"[INFO] Loading known face embeddings from {embeddings_file}\")\n",
    "    with open(embeddings_file, 'rb') as f:\n",
    "        database = pickle.load(f)\n",
    "\n",
    "# Compare detected face with known faces in the database\n",
    "for name, known_embedding in database.items():\n",
    "    if recognizer.compare_embeddings(embedding, known_embedding):\n",
    "        print(f\"[INFO] Recognized {name}\")\n",
    "        # Annotate the recognized face in the video feed\n",
    "        cv2.putText(image, f\"{name}\", (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        # Display the video feed with annotations\n",
    "\n",
    "cv2.imshow('PiPresence - Attendance Recognition', image)\n",
    "# Exit loop if 'q' is pressed\n",
    "if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "    print(\"[INFO] 'q' pressed, exiting the application\")\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading YOLOv8n face-model from ../data/models/yolov8n-face.onnx\n",
      "[INFO] Loading MobileFaceNet model from ../data/models/mobilefacenet_fixed.onnx\n",
      "[INFO] Processing ../data/images/tom/left.jpg\n",
      "[ERROR] At least one dimension is smaller than 640\n",
      "[ERROR] Failed to process ../data/images/tom/left.jpg\n",
      "[INFO] Processing ../data/images/tom/front.jpg\n",
      "[ERROR] Face recognition failed: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Got invalid dimensions for input: input:0 for the following indices\n",
      " index: 1 Got: 3 Expected: 112\n",
      " index: 3 Got: 112 Expected: 3\n",
      " Please fix either the inputs/outputs or the model.\n",
      "[INFO] Saved processed face to ../data/known_faces/tom/front.jpg\n",
      "[INFO] Processing ../data/images/tom/right.jpg\n",
      "[ERROR] Face recognition failed: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Got invalid dimensions for input: input:0 for the following indices\n",
      " index: 1 Got: 3 Expected: 112\n",
      " index: 3 Got: 112 Expected: 3\n",
      " Please fix either the inputs/outputs or the model.\n",
      "[INFO] Saved processed face to ../data/known_faces/tom/right.jpg\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'NoneType' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 10\u001b[0m\n\u001b[1;32m      3\u001b[0m Config\u001b[38;5;241m.\u001b[39mupdate_config(\n\u001b[1;32m      4\u001b[0m     yolo_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/models/yolov8n-face.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     mobilefacenet_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/models/mobilefacenet_fixed.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     input_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/images/\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m     output_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/known_faces\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m processor \u001b[38;5;241m=\u001b[39m ImagePreprocessor()\n\u001b[0;32m---> 10\u001b[0m \u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_database_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PiPresence/pipresence/preprocess.py:106\u001b[0m, in \u001b[0;36mImagePreprocessor.process_database_images\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m         error_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m embeddings:\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m# Average the embeddings from different profiles to get a more robust representation\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     average_embedding \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     database[person_name] \u001b[38;5;241m=\u001b[39m average_embedding\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INFO] Added \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mperson_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to the known faces database\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/PiPresence/.venv/lib/python3.10/site-packages/numpy/_core/fromnumeric.py:3596\u001b[0m, in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m   3593\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3594\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m mean(axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 3596\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_methods\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mean\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3597\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PiPresence/.venv/lib/python3.10/site-packages/numpy/_core/_methods.py:127\u001b[0m, in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m    124\u001b[0m         dtype \u001b[38;5;241m=\u001b[39m mu\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf4\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    125\u001b[0m         is_float16_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43mumr_sum\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, mu\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _no_nep50_warning():\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'NoneType' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "from pipresence.preprocess import ImagePreprocessor\n",
    "from pipresence.config import Config\n",
    "Config.update_config(\n",
    "    yolo_model_path = \"../data/models/yolov8n-face.onnx\",\n",
    "    mobilefacenet_model_path = \"../data/models/mobilefacenet_fixed.onnx\",\n",
    "    input_directory = \"../data/images/\",\n",
    "    output_directory = \"../data/known_faces\"\n",
    ")\n",
    "processor = ImagePreprocessor()\n",
    "processor.process_database_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'you': 'you', 'me': 'me', 'he': 'he'}\n",
      "(['you', 'me', 'she'], {1: '1', 2: '2', 3: '3'})\n"
     ]
    }
   ],
   "source": [
    "def func(**kwargs):\n",
    "    print(kwargs)\n",
    "\n",
    "func(you=\"you\", me=\"me\", he=\"he\")\n",
    "\n",
    "def func1(*args):\n",
    "    print(args)\n",
    "\n",
    "func1([\"you\", \"me\", \"she\"], {1:\"1\", 2: \"2\", 3: \"3\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eyes are cute\n",
      "Hair is long\n",
      "Height is 176\n",
      "{'eyes': 'sharp', 'hair': 'short', 'height': 173}\n",
      "dict_items([('eyes', 'sharp'), ('hair', 'short'), ('height', 173)])\n",
      "Eyes are sharp\n",
      "Hair is short\n",
      "Height is 173\n"
     ]
    }
   ],
   "source": [
    "class Person:\n",
    "    eyes = \"cute\"\n",
    "    hair = \"long\"\n",
    "    height = 176\n",
    "\n",
    "    @classmethod\n",
    "    def update_vars(cls, **kwargs):\n",
    "        print(kwargs)\n",
    "        print(kwargs.items())\n",
    "        for key, value in kwargs.items():\n",
    "            if hasattr(cls, key):\n",
    "                setattr(cls, key, value)\n",
    "            else:\n",
    "                print(f\"No such variable as {key}\")\n",
    "\n",
    "    @classmethod\n",
    "    def display_vars(cls):\n",
    "        print(f\"Eyes are {cls.eyes}\")\n",
    "        print(f\"Hair is {cls.hair}\")\n",
    "        print(f\"Height is {cls.height}\")\n",
    "\n",
    "Person.display_vars()\n",
    "Person.update_vars(**{\n",
    "        \"eyes\": \"sharp\",\n",
    "        \"hair\": \"short\",\n",
    "        \"height\": 173\n",
    "    }\n",
    ")\n",
    "Person.display_vars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
