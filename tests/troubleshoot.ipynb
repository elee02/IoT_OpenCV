{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Preprocessing image for face detection\n",
      "[INFO] Running inference on the image\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "session = ort.InferenceSession(\"../data/models/yolov8n.onnx\")\n",
    "input_name = session.get_inputs()[0].name\n",
    "\n",
    "image = cv2.imread(\"../data/test_images/common.jpg\")\n",
    "\n",
    "# Preprocess image for ONNX model\n",
    "print(\"[INFO] Preprocessing image for face detection\")\n",
    "# Resize image to the required size for YOLO model\n",
    "input_image = cv2.resize(image, (640, 640))\n",
    "# Normalize pixel values to range [0, 1]\n",
    "input_image = input_image.astype(np.float32) / 255.0\n",
    "# Change image layout to channel-first format as required by ONNX model\n",
    "input_image = np.transpose(input_image, (2, 0, 1))  # Channel first\n",
    "# Add batch dimension (needed by the model)\n",
    "input_image = np.expand_dims(input_image, axis=0)\n",
    "\n",
    "# Perform inference on the provided image\n",
    "print(\"[INFO] Running inference on the image\")\n",
    "\n",
    "output = session.run(\n",
    "    output_names=None, \n",
    "    input_feed= {input_name: input_image}\n",
    ")\n",
    "outputs = output[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 22.25124 ,  63.748386,  87.80733 , 129.69324 , 174.27881 ,\n",
       "       192.77115 , 213.32562 , 269.3759  , 316.48227 , 322.21863 ,\n",
       "       320.0763  , 316.99564 , 318.22125 , 326.87085 , 433.69553 ,\n",
       "       461.33926 , 492.1537  , 521.9314  , 574.0167  , 586.5244  ,\n",
       "        22.237974,  64.08118 , 112.29887 , 154.78146 , 179.81332 ,\n",
       "       190.1046  , 224.40012 , 279.797   , 318.0047  , 318.1396  ,\n",
       "       320.30872 , 317.26102 , 318.1406  , 335.68372 , 430.21753 ,\n",
       "       459.9945  , 492.00806 , 498.9185  , 567.9201  , 587.4642  ,\n",
       "        22.73684 ,  68.41667 , 181.43848 , 173.88988 , 179.5802  ,\n",
       "       191.43561 , 238.93619 , 288.25153 , 318.90552 , 317.70673 ,\n",
       "       321.46643 , 318.6952  , 319.8886  , 353.45447 , 402.1404  ,\n",
       "       459.18643 , 494.7537  , 490.04715 , 542.8994  , 588.4896  ,\n",
       "        22.670723,  91.90434 , 216.26353 , 197.72499 , 181.47786 ,\n",
       "       196.88611 , 270.8847  , 307.2757  , 320.03256 , 318.68646 ,\n",
       "       323.23904 , 320.78094 , 317.70187 , 334.6641  , 363.4209  ,\n",
       "       436.91595 , 483.63205 , 472.1972  , 524.3074  , 590.328   ,\n",
       "        24.313564, 143.87476 , 197.07613 , 214.93752 , 202.09161 ,\n",
       "       218.15901 , 282.89044 , 311.0667  , 319.37445 , 317.96368 ,\n",
       "       324.19092 , 313.88373 , 314.94144 , 326.45734 , 349.84894 ,\n",
       "       391.12936 , 453.38788 , 472.55856 , 497.5859  , 574.4389  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0][8300:8400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces = []\n",
    "\n",
    "for detection in outputs:\n",
    "    # Extract bounding box coordinates\n",
    "    center_x, center_y, width, height = detection[:4]\n",
    "\n",
    "    # Denormalize to original image size (assuming image size of 640x640)\n",
    "    center_x *= 640\n",
    "    center_y *= 640\n",
    "    width *= 640\n",
    "    height *= 640\n",
    "\n",
    "    # Convert to (x1, y1, x2, y2)\n",
    "    x1 = int(center_x - width / 2)\n",
    "    y1 = int(center_y - height / 2)\n",
    "    x2 = int(center_x + width / 2)\n",
    "    y2 = int(center_y + height / 2)\n",
    "\n",
    "    # Extract objectness score\n",
    "    objectness_score = detection[4]\n",
    "    confidence_threshold = 0.65\n",
    "    if objectness_score < confidence_threshold:\n",
    "        continue\n",
    "\n",
    "    # Extract class scores and determine the class with highest confidence\n",
    "    class_scores = detection[5:]\n",
    "    class_id = np.argmax(class_scores)\n",
    "    class_confidence = class_scores[class_id]\n",
    "\n",
    "    # Set a class confidence threshold\n",
    "    if class_confidence > 0.5:\n",
    "        faces.append([x1, y1, x2, y2, class_id, class_confidence])\n",
    "        # Optionally draw the bounding box on the image\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), (255, 0, 0), 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_OpNamespace' 'torchvision' object has no attribute 'nms'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 57\u001b[0m\n\u001b[1;32m     55\u001b[0m boxes_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(boxes)\n\u001b[1;32m     56\u001b[0m confidences_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(confidences)\n\u001b[0;32m---> 57\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnms\u001b[49m(boxes_tensor, confidences_tensor, iou_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m     58\u001b[0m boxes \u001b[38;5;241m=\u001b[39m boxes[indices]\n\u001b[1;32m     59\u001b[0m confidences \u001b[38;5;241m=\u001b[39m confidences[indices]\n",
      "File \u001b[0;32m~/PiPresence/.venv/lib/python3.10/site-packages/torch/_ops.py:1225\u001b[0m, in \u001b[0;36m_OpNamespace.__getattr__\u001b[0;34m(self, op_name)\u001b[0m\n\u001b[1;32m   1223\u001b[0m     op, overload_names \u001b[38;5;241m=\u001b[39m _get_packet(qualified_op_name, module_name)\n\u001b[1;32m   1224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m op \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1225\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1226\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_OpNamespace\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mop_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1227\u001b[0m         )\n\u001b[1;32m   1228\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1229\u001b[0m     \u001b[38;5;66;03m# Turn this into AttributeError so getattr(obj, key, default)\u001b[39;00m\n\u001b[1;32m   1230\u001b[0m     \u001b[38;5;66;03m# works (this is called by TorchScript with __origin__)\u001b[39;00m\n\u001b[1;32m   1231\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1232\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_OpNamespace\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mop_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1233\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_OpNamespace' 'torchvision' object has no attribute 'nms'"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Load the model\n",
    "session = ort.InferenceSession(\"../data/models/yolov8n.onnx\")\n",
    "input_name = session.get_inputs()[0].name\n",
    "\n",
    "# Load and preprocess the image\n",
    "image = cv2.imread(\"../data/test_images/common.jpg\")\n",
    "input_image = cv2.resize(image, (640, 640))\n",
    "input_image = input_image.astype(np.float32) / 255.0\n",
    "input_image = np.transpose(input_image, (2, 0, 1))  # Channel first\n",
    "input_image = np.expand_dims(input_image, axis=0)\n",
    "\n",
    "# Run inference\n",
    "output = session.run(None, {input_name: input_image})\n",
    "\n",
    "# Process the outputs\n",
    "outputs = output[0]\n",
    "outputs = outputs.transpose(0, 2, 1)\n",
    "outputs = outputs[0]\n",
    "\n",
    "# Extract boxes and scores\n",
    "boxes = outputs[:, :4]\n",
    "scores = outputs[:, 4:]\n",
    "\n",
    "# Apply activation functions\n",
    "class_probs = 1 / (1 + np.exp(-scores))\n",
    "\n",
    "# Decode bounding boxes\n",
    "boxes[:, 0] = boxes[:, 0] - boxes[:, 2] / 2  # x1\n",
    "boxes[:, 1] = boxes[:, 1] - boxes[:, 3] / 2  # y1\n",
    "boxes[:, 2] = boxes[:, 0] + boxes[:, 2]      # x2\n",
    "boxes[:, 3] = boxes[:, 1] + boxes[:, 3]      # y2\n",
    "\n",
    "# Filter predictions\n",
    "confidences = np.max(class_probs, axis=1)\n",
    "class_ids = np.argmax(class_probs, axis=1)\n",
    "conf_threshold = 0.5\n",
    "mask = confidences > conf_threshold\n",
    "boxes = boxes[mask]\n",
    "confidences = confidences[mask]\n",
    "class_ids = class_ids[mask]\n",
    "\n",
    "# Rescale boxes\n",
    "orig_height, orig_width = image.shape[:2]\n",
    "scale_x = orig_width / 640\n",
    "scale_y = orig_height / 640\n",
    "boxes[:, [0, 2]] *= scale_x\n",
    "boxes[:, [1, 3]] *= scale_y\n",
    "\n",
    "# Apply NMS\n",
    "boxes_tensor = torch.tensor(boxes)\n",
    "confidences_tensor = torch.tensor(confidences)\n",
    "indices = torch.ops.torchvision.nms(boxes_tensor, confidences_tensor, iou_threshold=0.5)\n",
    "boxes = boxes[indices]\n",
    "confidences = confidences[indices]\n",
    "class_ids = class_ids[indices]\n",
    "\n",
    "# Draw boxes on the image\n",
    "for box, conf, class_id in zip(boxes, confidences, class_ids):\n",
    "    x1, y1, x2, y2 = box.astype(int)\n",
    "    label = f\"Class {class_id}: {conf:.2f}\"\n",
    "    cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "    cv2.putText(image, label, (x1, y1 - 10),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "# Display the image\n",
    "cv2.imshow(\"Detections\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ../data/models/yolov8n.onnx for ONNX Runtime inference...\n",
      "WARNING âš ï¸ Failed to start ONNX Runtime session with CUDA. Falling back to CPU...\n",
      "Preferring ONNX Runtime AzureExecutionProvider\n",
      "\n",
      "image 1/1 /home/el02/PiPresence/tests/../data/known_faces/elyor/front.jpg: 640x640 1 person, 33.7ms\n",
      "Speed: 0.7ms preprocess, 33.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"../data/models/yolov8n.onnx\", task=\"detect\")\n",
    "results = model(\"../data/known_faces/elyor/front.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer names:\n",
      "['', 'model', 'model.0', 'model.0.conv', 'model.0.bn', 'model.0.act', 'model.1', 'model.1.conv', 'model.1.bn', 'model.2', 'model.2.cv1', 'model.2.cv1.conv', 'model.2.cv1.bn', 'model.2.cv2', 'model.2.cv2.conv', 'model.2.cv2.bn', 'model.2.m', 'model.2.m.0', 'model.2.m.0.cv1', 'model.2.m.0.cv1.conv', 'model.2.m.0.cv1.bn', 'model.2.m.0.cv2', 'model.2.m.0.cv2.conv', 'model.2.m.0.cv2.bn', 'model.3', 'model.3.conv', 'model.3.bn', 'model.4', 'model.4.cv1', 'model.4.cv1.conv', 'model.4.cv1.bn', 'model.4.cv2', 'model.4.cv2.conv', 'model.4.cv2.bn', 'model.4.m', 'model.4.m.0', 'model.4.m.0.cv1', 'model.4.m.0.cv1.conv', 'model.4.m.0.cv1.bn', 'model.4.m.0.cv2', 'model.4.m.0.cv2.conv', 'model.4.m.0.cv2.bn', 'model.4.m.1', 'model.4.m.1.cv1', 'model.4.m.1.cv1.conv', 'model.4.m.1.cv1.bn', 'model.4.m.1.cv2', 'model.4.m.1.cv2.conv', 'model.4.m.1.cv2.bn', 'model.5', 'model.5.conv', 'model.5.bn', 'model.6', 'model.6.cv1', 'model.6.cv1.conv', 'model.6.cv1.bn', 'model.6.cv2', 'model.6.cv2.conv', 'model.6.cv2.bn', 'model.6.m', 'model.6.m.0', 'model.6.m.0.cv1', 'model.6.m.0.cv1.conv', 'model.6.m.0.cv1.bn', 'model.6.m.0.cv2', 'model.6.m.0.cv2.conv', 'model.6.m.0.cv2.bn', 'model.6.m.1', 'model.6.m.1.cv1', 'model.6.m.1.cv1.conv', 'model.6.m.1.cv1.bn', 'model.6.m.1.cv2', 'model.6.m.1.cv2.conv', 'model.6.m.1.cv2.bn', 'model.7', 'model.7.conv', 'model.7.bn', 'model.8', 'model.8.cv1', 'model.8.cv1.conv', 'model.8.cv1.bn', 'model.8.cv2', 'model.8.cv2.conv', 'model.8.cv2.bn', 'model.8.m', 'model.8.m.0', 'model.8.m.0.cv1', 'model.8.m.0.cv1.conv', 'model.8.m.0.cv1.bn', 'model.8.m.0.cv2', 'model.8.m.0.cv2.conv', 'model.8.m.0.cv2.bn', 'model.9', 'model.9.cv1', 'model.9.cv1.conv', 'model.9.cv1.bn', 'model.9.cv2', 'model.9.cv2.conv', 'model.9.cv2.bn', 'model.9.m', 'model.10', 'model.11', 'model.12', 'model.12.cv1', 'model.12.cv1.conv', 'model.12.cv1.bn', 'model.12.cv2', 'model.12.cv2.conv', 'model.12.cv2.bn', 'model.12.m', 'model.12.m.0', 'model.12.m.0.cv1', 'model.12.m.0.cv1.conv', 'model.12.m.0.cv1.bn', 'model.12.m.0.cv2', 'model.12.m.0.cv2.conv', 'model.12.m.0.cv2.bn', 'model.13', 'model.14', 'model.15', 'model.15.cv1', 'model.15.cv1.conv', 'model.15.cv1.bn', 'model.15.cv2', 'model.15.cv2.conv', 'model.15.cv2.bn', 'model.15.m', 'model.15.m.0', 'model.15.m.0.cv1', 'model.15.m.0.cv1.conv', 'model.15.m.0.cv1.bn', 'model.15.m.0.cv2', 'model.15.m.0.cv2.conv', 'model.15.m.0.cv2.bn', 'model.16', 'model.16.conv', 'model.16.bn', 'model.17', 'model.18', 'model.18.cv1', 'model.18.cv1.conv', 'model.18.cv1.bn', 'model.18.cv2', 'model.18.cv2.conv', 'model.18.cv2.bn', 'model.18.m', 'model.18.m.0', 'model.18.m.0.cv1', 'model.18.m.0.cv1.conv', 'model.18.m.0.cv1.bn', 'model.18.m.0.cv2', 'model.18.m.0.cv2.conv', 'model.18.m.0.cv2.bn', 'model.19', 'model.19.conv', 'model.19.bn', 'model.20', 'model.21', 'model.21.cv1', 'model.21.cv1.conv', 'model.21.cv1.bn', 'model.21.cv2', 'model.21.cv2.conv', 'model.21.cv2.bn', 'model.21.m', 'model.21.m.0', 'model.21.m.0.cv1', 'model.21.m.0.cv1.conv', 'model.21.m.0.cv1.bn', 'model.21.m.0.cv2', 'model.21.m.0.cv2.conv', 'model.21.m.0.cv2.bn', 'model.22', 'model.22.cv2', 'model.22.cv2.0', 'model.22.cv2.0.0', 'model.22.cv2.0.0.conv', 'model.22.cv2.0.0.bn', 'model.22.cv2.0.1', 'model.22.cv2.0.1.conv', 'model.22.cv2.0.1.bn', 'model.22.cv2.0.2', 'model.22.cv2.1', 'model.22.cv2.1.0', 'model.22.cv2.1.0.conv', 'model.22.cv2.1.0.bn', 'model.22.cv2.1.1', 'model.22.cv2.1.1.conv', 'model.22.cv2.1.1.bn', 'model.22.cv2.1.2', 'model.22.cv2.2', 'model.22.cv2.2.0', 'model.22.cv2.2.0.conv', 'model.22.cv2.2.0.bn', 'model.22.cv2.2.1', 'model.22.cv2.2.1.conv', 'model.22.cv2.2.1.bn', 'model.22.cv2.2.2', 'model.22.cv3', 'model.22.cv3.0', 'model.22.cv3.0.0', 'model.22.cv3.0.0.conv', 'model.22.cv3.0.0.bn', 'model.22.cv3.0.1', 'model.22.cv3.0.1.conv', 'model.22.cv3.0.1.bn', 'model.22.cv3.0.2', 'model.22.cv3.1', 'model.22.cv3.1.0', 'model.22.cv3.1.0.conv', 'model.22.cv3.1.0.bn', 'model.22.cv3.1.1', 'model.22.cv3.1.1.conv', 'model.22.cv3.1.1.bn', 'model.22.cv3.1.2', 'model.22.cv3.2', 'model.22.cv3.2.0', 'model.22.cv3.2.0.conv', 'model.22.cv3.2.0.bn', 'model.22.cv3.2.1', 'model.22.cv3.2.1.conv', 'model.22.cv3.2.1.bn', 'model.22.cv3.2.2', 'model.22.dfl', 'model.22.dfl.conv']\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the model\n",
    "model = YOLO('../data/models/yolov8n.pt')\n",
    "\n",
    "# Get the model architecture (Pytorch model)\n",
    "yolo_model = model.model\n",
    "\n",
    "# Get the names of all layers\n",
    "layer_names = [name for name, _ in yolo_model.named_modules()]\n",
    "\n",
    "print(\"Layer names:\")\n",
    "print(layer_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/el02/PiPresence/.venv/lib/python3.10/site-packages/ultralytics/nn/modules/head.py:105: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if self.format != \"imx\" and (self.dynamic or self.shape != shape):\n",
      "/home/el02/PiPresence/.venv/lib/python3.10/site-packages/ultralytics/utils/tal.py:308: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n",
      "  for i, stride in enumerate(strides):\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define a dummy input\n",
    "dummy_input = torch.randn(1, 3, 640, 640)\n",
    "\n",
    "# Export to ONNX\n",
    "torch.onnx.export(yolo_model, dummy_input, \"data/models/yolov8n.onnx\",\n",
    "                  input_names=['input'],\n",
    "                  output_names=['output_boxes', 'output_classes'],\n",
    "                  dynamic_axes={'input': {0: 'batch_size'}, 'output_boxes': {0: 'batch_size'}, 'output_classes': {0: 'batch_size'}})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Preprocessing image for face detection\n",
      "[INFO] Running inference on the image\n",
      "Output shape: (1, 5, 8400)\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the ONNX model\n",
    "session = ort.InferenceSession(\"../data/models/yolov8n-face.onnx\")\n",
    "\n",
    "# Get the input name for ONNX model\n",
    "input_name = session.get_inputs()[0].name\n",
    "\n",
    "# Get the output names from ONNX model\n",
    "output_names = [output.name for output in session.get_outputs()]\n",
    "\n",
    "# Load and preprocess the image\n",
    "image = cv2.imread(\"../data/test_images/friends_gathering.jpg\")\n",
    "print(\"[INFO] Preprocessing image for face detection\")\n",
    "input_image = cv2.resize(image, (640, 640))\n",
    "input_image = input_image.astype(np.float32) / 255.0\n",
    "input_image = np.transpose(input_image, (2, 0, 1))  # Channel first\n",
    "input_image = np.expand_dims(input_image, axis=0)\n",
    "\n",
    "# Perform inference on the provided image\n",
    "print(\"[INFO] Running inference on the image\")\n",
    "outputs = session.run(output_names, {input_name: input_image})\n",
    "\n",
    "# Output shape is likely (1, 84, 8400)\n",
    "print(\"Output shape:\", outputs[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ultralytics YOLO ðŸš€, AGPL-3.0 license\n",
    "\n",
    "import argparse\n",
    "\n",
    "import cv2.dnn\n",
    "import numpy as np\n",
    "\n",
    "from ultralytics.utils import ASSETS, yaml_load\n",
    "from ultralytics.utils.checks import check_yaml\n",
    "\n",
    "CLASSES = yaml_load(check_yaml(\"coco8.yaml\"))[\"names\"]\n",
    "colors = np.random.uniform(0, 255, size=(len(CLASSES), 3))\n",
    "\n",
    "\n",
    "def draw_bounding_box(img, class_id, confidence, x, y, x_plus_w, y_plus_h):\n",
    "    \"\"\"\n",
    "    Draws bounding boxes on the input image based on the provided arguments.\n",
    "\n",
    "    Args:\n",
    "        img (numpy.ndarray): The input image to draw the bounding box on.\n",
    "        class_id (int): Class ID of the detected object.\n",
    "        confidence (float): Confidence score of the detected object.\n",
    "        x (int): X-coordinate of the top-left corner of the bounding box.\n",
    "        y (int): Y-coordinate of the top-left corner of the bounding box.\n",
    "        x_plus_w (int): X-coordinate of the bottom-right corner of the bounding box.\n",
    "        y_plus_h (int): Y-coordinate of the bottom-right corner of the bounding box.\n",
    "    \"\"\"\n",
    "    label = f\"{CLASSES[class_id]} ({confidence:.2f})\"\n",
    "    color = colors[class_id]\n",
    "    cv2.rectangle(img, (x, y), (x_plus_w, y_plus_h), color, 2)\n",
    "    cv2.putText(img, label, (x - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "\n",
    "def main(onnx_model, input_image):\n",
    "    \"\"\"\n",
    "    Main function to load ONNX model, perform inference, draw bounding boxes, and display the output image.\n",
    "\n",
    "    Args:\n",
    "        onnx_model (str): Path to the ONNX model.\n",
    "        input_image (str): Path to the input image.\n",
    "\n",
    "    Returns:\n",
    "        list: List of dictionaries containing detection information such as class_id, class_name, confidence, etc.\n",
    "    \"\"\"\n",
    "    # Load the ONNX model\n",
    "    model: cv2.dnn.Net = cv2.dnn.readNetFromONNX(onnx_model)\n",
    "\n",
    "    # Read the input image\n",
    "    original_image: np.ndarray = cv2.imread(input_image)\n",
    "    [height, width, _] = original_image.shape\n",
    "\n",
    "    # Prepare a square image for inference\n",
    "    length = max((height, width))\n",
    "    image = np.zeros((length, length, 3), np.uint8)\n",
    "    image[0:height, 0:width] = original_image\n",
    "\n",
    "    # Calculate scale factor\n",
    "    scale = length / 640\n",
    "\n",
    "    # Preprocess the image and prepare blob for model\n",
    "    blob = cv2.dnn.blobFromImage(image, scalefactor=1 / 255, size=(640, 640), swapRB=True)\n",
    "    model.setInput(blob)\n",
    "\n",
    "    # Perform inference\n",
    "    outputs = model.forward()\n",
    "\n",
    "    # Prepare output array\n",
    "    outputs = np.array([cv2.transpose(outputs[0])])\n",
    "    rows = outputs.shape[1]\n",
    "\n",
    "    boxes = []\n",
    "    scores = []\n",
    "    class_ids = []\n",
    "\n",
    "    # Iterate through output to collect bounding boxes, confidence scores, and class IDs\n",
    "    for i in range(rows):\n",
    "        classes_scores = outputs[0][i][4:]\n",
    "        (minScore, maxScore, minClassLoc, (x, maxClassIndex)) = cv2.minMaxLoc(classes_scores)\n",
    "        if maxScore >= 0.25:\n",
    "            box = [\n",
    "                outputs[0][i][0] - (0.5 * outputs[0][i][2]),\n",
    "                outputs[0][i][1] - (0.5 * outputs[0][i][3]),\n",
    "                outputs[0][i][2],\n",
    "                outputs[0][i][3],\n",
    "            ]\n",
    "            boxes.append(box)\n",
    "            scores.append(maxScore)\n",
    "            class_ids.append(maxClassIndex)\n",
    "\n",
    "    # Apply NMS (Non-maximum suppression)\n",
    "    result_boxes = cv2.dnn.NMSBoxes(boxes, scores, 0.25, 0.45, 0.5)\n",
    "\n",
    "    detections = []\n",
    "\n",
    "    # Iterate through NMS results to draw bounding boxes and labels\n",
    "    for i in range(len(result_boxes)):\n",
    "        index = result_boxes[i]\n",
    "        box = boxes[index]\n",
    "        detection = {\n",
    "            \"class_id\": class_ids[index],\n",
    "            \"class_name\": CLASSES[class_ids[index]],\n",
    "            \"confidence\": scores[index],\n",
    "            \"box\": box,\n",
    "            \"scale\": scale,\n",
    "        }\n",
    "        detections.append(detection)\n",
    "        draw_bounding_box(\n",
    "            original_image,\n",
    "            class_ids[index],\n",
    "            scores[index],\n",
    "            round(box[0] * scale),\n",
    "            round(box[1] * scale),\n",
    "            round((box[0] + box[2]) * scale),\n",
    "            round((box[1] + box[3]) * scale),\n",
    "        )\n",
    "\n",
    "    # Display the image with bounding boxes\n",
    "    cv2.imshow(\"image\", original_image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'class_id': 0,\n",
       "  'class_name': 'person',\n",
       "  'confidence': 0.9291252493858337,\n",
       "  'box': [np.float32(72.9223),\n",
       "   np.float32(2.3677368),\n",
       "   np.float32(496.26514),\n",
       "   np.float32(353.2938)],\n",
       "  'scale': 1.875}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main(\"../data/models/yolov8n.onnx\", \"../data/test_images/common.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading YOLOv8n face-model from ../data/models/yolov8n-face.onnx\n"
     ]
    }
   ],
   "source": [
    "from pipresence.detect_faces import FaceDetector\n",
    "import cv2 \n",
    "\n",
    "detector = FaceDetector(\"../data/models/yolov8n-face.onnx\")\n",
    "image = cv2.imread(\"../data/known_faces/elyor/right.jpg\")\n",
    "detections = detector.detect_faces(image)\n",
    "detection = detections[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'class_id': 0, 'class_name': 'person', 'confidence': 0.3326736390590668, 'box': [np.float32(195.62439), np.float32(135.87161), np.float32(293.73694), np.float32(217.464)], 'scale': 1.0}]\n",
      "[INFO] 'q' pressed, exiting the application\n"
     ]
    }
   ],
   "source": [
    "print(detections)\n",
    "bbox = detection[\"box\"]\n",
    "x = round(bbox[0] * detection[\"scale\"])\n",
    "y = round(bbox[1] * detection[\"scale\"])\n",
    "x_plus_w = round((bbox[0] + bbox[2]) * detection[\"scale\"])\n",
    "y_plus_h = round((bbox[1] + bbox[3]) * detection[\"scale\"])\n",
    "color = (200, 56, 159)\n",
    "cv2.rectangle(image, (x, y), (x_plus_w, y_plus_h), color, 2)\n",
    "cv2.putText(image, \"Common\", (x - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "while True:\n",
    "    cv2.imshow(\"Example\", image)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        print(\"[INFO] 'q' pressed, exiting the application\")\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading MobileFaceNet model from ../data/models/mobilefacenet_fixed.onnx\n"
     ]
    },
    {
     "ename": "InvalidArgument",
     "evalue": "[ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Got invalid dimensions for input: input:0 for the following indices\n index: 1 Got: 3 Expected: 112\n index: 2 Got: 640 Expected: 112\n index: 3 Got: 640 Expected: 3\n Please fix either the inputs/outputs or the model.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgument\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m preprocessed_image \u001b[38;5;241m=\u001b[39m recognizer\u001b[38;5;241m.\u001b[39mpreprocess(image)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Run inference\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mrecognizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mrecognizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocessed_image\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PiPresence/.venv/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:266\u001b[0m, in \u001b[0;36mSession.run\u001b[0;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[1;32m    264\u001b[0m     output_names \u001b[38;5;241m=\u001b[39m [output\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs_meta]\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_feed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m C\u001b[38;5;241m.\u001b[39mEPFail \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_fallback:\n",
      "\u001b[0;31mInvalidArgument\u001b[0m: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Got invalid dimensions for input: input:0 for the following indices\n index: 1 Got: 3 Expected: 112\n index: 2 Got: 640 Expected: 112\n index: 3 Got: 640 Expected: 3\n Please fix either the inputs/outputs or the model."
     ]
    }
   ],
   "source": [
    "from pipresence.recognize_faces import FaceRecognizer\n",
    "from pipresence.config import Config \n",
    "import cv2\n",
    "\n",
    "Config.update_config(mobilefacenet_model_path=\"../data/models/mobilefacenet_fixed.onnx\")\n",
    "recognizer = FaceRecognizer()\n",
    "image = cv2.imread(\"../data/known_faces/tom/front.jpg\")\n",
    "preprocessed_image = recognizer.preprocess(image)\n",
    "\n",
    "# Run inference\n",
    "outputs = recognizer.session.run(None, {recognizer.input_name: preprocessed_image})\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading MobileFaceNet model from ../data/models/mobilefacenet0.onnx\n",
      "[ERROR] Face recognition failed: 'dict' object has no attribute 'shape'\n"
     ]
    }
   ],
   "source": [
    "from pipresence.recognize_faces import FaceRecognizer\n",
    "import cv2\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "recognizer = FaceRecognizer(\"../data/models/mobilefacenet0.onnx\")\n",
    "embeddings_file = \"../data/encodings/face_embeddings.pkl\"\n",
    "image = cv2.imread(\"../data/known_faces/elyor/right.jpg\")\n",
    "embedding = recognizer.recognize_face(detections[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading known face embeddings from ../data/encodings/face_embeddings.pkl\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "if os.path.exists(embeddings_file):\n",
    "    # Load existing embeddings from the file\n",
    "    print(f\"[INFO] Loading known face embeddings from {embeddings_file}\")\n",
    "    with open(embeddings_file, 'rb') as f:\n",
    "        database = pickle.load(f)\n",
    "\n",
    "# Compare detected face with known faces in the database\n",
    "for name, known_embedding in database.items():\n",
    "    if recognizer.compare_embeddings(embedding, known_embedding):\n",
    "        print(f\"[INFO] Recognized {name}\")\n",
    "        # Annotate the recognized face in the video feed\n",
    "        cv2.putText(image, f\"{name}\", (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        # Display the video feed with annotations\n",
    "\n",
    "cv2.imshow('PiPresence - Attendance Recognition', image)\n",
    "# Exit loop if 'q' is pressed\n",
    "if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "    print(\"[INFO] 'q' pressed, exiting the application\")\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading YOLOv8n face-model from ../data/models/yolov8n-face.onnx\n",
      "[INFO] Loading MobileFaceNet model from ../data/models/mobilefacenet_fixed.onnx\n",
      "[INFO] Processing ../data/images/tom/left.jpg\n",
      "[ERROR] At least one dimension is smaller than 640\n",
      "[ERROR] Failed to process ../data/images/tom/left.jpg\n",
      "[INFO] Processing ../data/images/tom/front.jpg\n",
      "[ERROR] Face recognition failed: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Got invalid dimensions for input: input:0 for the following indices\n",
      " index: 1 Got: 3 Expected: 112\n",
      " index: 3 Got: 112 Expected: 3\n",
      " Please fix either the inputs/outputs or the model.\n",
      "[INFO] Saved processed face to ../data/known_faces/tom/front.jpg\n",
      "[INFO] Processing ../data/images/tom/right.jpg\n",
      "[ERROR] Face recognition failed: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Got invalid dimensions for input: input:0 for the following indices\n",
      " index: 1 Got: 3 Expected: 112\n",
      " index: 3 Got: 112 Expected: 3\n",
      " Please fix either the inputs/outputs or the model.\n",
      "[INFO] Saved processed face to ../data/known_faces/tom/right.jpg\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'NoneType' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 10\u001b[0m\n\u001b[1;32m      3\u001b[0m Config\u001b[38;5;241m.\u001b[39mupdate_config(\n\u001b[1;32m      4\u001b[0m     yolo_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/models/yolov8n-face.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     mobilefacenet_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/models/mobilefacenet_fixed.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     input_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/images/\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m     output_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/known_faces\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m processor \u001b[38;5;241m=\u001b[39m ImagePreprocessor()\n\u001b[0;32m---> 10\u001b[0m \u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_database_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PiPresence/pipresence/preprocess.py:106\u001b[0m, in \u001b[0;36mImagePreprocessor.process_database_images\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m         error_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m embeddings:\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m# Average the embeddings from different profiles to get a more robust representation\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     average_embedding \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     database[person_name] \u001b[38;5;241m=\u001b[39m average_embedding\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INFO] Added \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mperson_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to the known faces database\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/PiPresence/.venv/lib/python3.10/site-packages/numpy/_core/fromnumeric.py:3596\u001b[0m, in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m   3593\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3594\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m mean(axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 3596\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_methods\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mean\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3597\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PiPresence/.venv/lib/python3.10/site-packages/numpy/_core/_methods.py:127\u001b[0m, in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m    124\u001b[0m         dtype \u001b[38;5;241m=\u001b[39m mu\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf4\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    125\u001b[0m         is_float16_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43mumr_sum\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, mu\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _no_nep50_warning():\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'NoneType' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "from pipresence.preprocess import ImagePreprocessor\n",
    "from pipresence.config import Config\n",
    "Config.update_config(\n",
    "    yolo_model_path = \"../data/models/yolov8n-face.onnx\",\n",
    "    mobilefacenet_model_path = \"../data/models/mobilefacenet_fixed.onnx\",\n",
    "    input_directory = \"../data/images/\",\n",
    "    output_directory = \"../data/known_faces\"\n",
    ")\n",
    "processor = ImagePreprocessor()\n",
    "processor.process_database_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'you': 'you', 'me': 'me', 'he': 'he'}\n",
      "(['you', 'me', 'she'], {1: '1', 2: '2', 3: '3'})\n"
     ]
    }
   ],
   "source": [
    "def func(**kwargs):\n",
    "    print(kwargs)\n",
    "\n",
    "func(you=\"you\", me=\"me\", he=\"he\")\n",
    "\n",
    "def func1(*args):\n",
    "    print(args)\n",
    "\n",
    "func1([\"you\", \"me\", \"she\"], {1:\"1\", 2: \"2\", 3: \"3\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eyes are cute\n",
      "Hair is long\n",
      "Height is 176\n",
      "{'eyes': 'sharp', 'hair': 'short', 'height': 173}\n",
      "dict_items([('eyes', 'sharp'), ('hair', 'short'), ('height', 173)])\n",
      "Eyes are sharp\n",
      "Hair is short\n",
      "Height is 173\n"
     ]
    }
   ],
   "source": [
    "class Person:\n",
    "    eyes = \"cute\"\n",
    "    hair = \"long\"\n",
    "    height = 176\n",
    "\n",
    "    @classmethod\n",
    "    def update_vars(cls, **kwargs):\n",
    "        print(kwargs)\n",
    "        print(kwargs.items())\n",
    "        for key, value in kwargs.items():\n",
    "            if hasattr(cls, key):\n",
    "                setattr(cls, key, value)\n",
    "            else:\n",
    "                print(f\"No such variable as {key}\")\n",
    "\n",
    "    @classmethod\n",
    "    def display_vars(cls):\n",
    "        print(f\"Eyes are {cls.eyes}\")\n",
    "        print(f\"Hair is {cls.hair}\")\n",
    "        print(f\"Height is {cls.height}\")\n",
    "\n",
    "Person.display_vars()\n",
    "Person.update_vars(**{\n",
    "        \"eyes\": \"sharp\",\n",
    "        \"hair\": \"short\",\n",
    "        \"height\": 173\n",
    "    }\n",
    ")\n",
    "Person.display_vars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Abubakr': array([-0.19784423, -0.00302832, -0.02722896, -0.05801077, -0.10105255,\n",
       "        -0.0967257 ,  0.08954489,  0.10473246, -0.04211131,  0.12898777,\n",
       "         0.07911804,  0.03632415, -0.10943891,  0.08779069,  0.00898741,\n",
       "        -0.05153709, -0.04161552, -0.10134095,  0.04073952,  0.03972577,\n",
       "        -0.02362769, -0.09986002, -0.003306  , -0.07574529, -0.03082445,\n",
       "        -0.01202941,  0.00125861,  0.02025795, -0.07998294,  0.15857612,\n",
       "        -0.12470126, -0.07703517, -0.00512415,  0.08261012, -0.04272592,\n",
       "         0.17545327, -0.14579589,  0.08747222, -0.02416262, -0.05242953,\n",
       "         0.06600434,  0.02335808, -0.09444121, -0.00544802,  0.01178771,\n",
       "        -0.05421624, -0.06772334, -0.11302577,  0.01366547,  0.06926548,\n",
       "        -0.01171857, -0.03055241, -0.05336759, -0.00622748,  0.01451244,\n",
       "         0.02659508, -0.03857574,  0.03061229,  0.15025533, -0.07283625,\n",
       "        -0.04514926,  0.01894576, -0.07179353, -0.09156753, -0.04327692,\n",
       "        -0.12064179,  0.02731288, -0.01485442, -0.02420474,  0.00036899,\n",
       "        -0.07941284,  0.01689505,  0.0319164 ,  0.03389412, -0.14403083,\n",
       "        -0.08293398,  0.01101746, -0.11071393, -0.057933  ,  0.05202797,\n",
       "         0.09930078,  0.29291049, -0.01042145,  0.02158281, -0.01771855,\n",
       "        -0.12263448,  0.02907261,  0.00806593, -0.07553181, -0.01131508,\n",
       "         0.04747841,  0.17801851,  0.07096314, -0.07444474, -0.00455204,\n",
       "        -0.04677681, -0.02684394,  0.14125886,  0.09267182,  0.00787985,\n",
       "        -0.05127251,  0.04018703,  0.06549724, -0.00970694,  0.02931298,\n",
       "         0.07038863,  0.0602466 , -0.11970025, -0.09692968,  0.08958936,\n",
       "        -0.07848842, -0.02708353, -0.08388055, -0.00888555,  0.06722627,\n",
       "        -0.14196286, -0.008278  , -0.06060725, -0.02177375, -0.11228942,\n",
       "         0.20014295,  0.09942424, -0.06176442,  0.03836173, -0.0907513 ,\n",
       "         0.04628272,  0.03667031, -0.12603854]),\n",
       " 'Shaxnoza': array([-9.79093164e-02, -3.75237935e-02,  1.08545877e-01,  3.29379858e-02,\n",
       "        -3.00397910e-02, -1.20215828e-01,  1.23824751e-01,  8.24781237e-02,\n",
       "        -5.75875286e-02,  1.80388429e-02,  1.06158961e-01,  1.01706977e-02,\n",
       "        -1.00815941e-01,  1.14417197e-01,  3.72375004e-02, -2.32422829e-03,\n",
       "        -7.04482035e-03, -7.56760519e-02,  4.87343417e-02,  9.09239766e-02,\n",
       "         2.18576100e-02, -4.22895784e-02, -9.79097672e-02, -1.28755044e-01,\n",
       "        -3.04052846e-02,  1.31523783e-02, -3.59256982e-02, -2.25666063e-02,\n",
       "        -1.20754009e-01,  1.31437048e-01, -6.70226812e-02, -1.02711175e-01,\n",
       "         2.38608690e-02,  1.19142678e-01, -2.07935409e-02,  9.17023607e-02,\n",
       "        -2.27571666e-01,  3.00853066e-02, -1.05814681e-02, -1.99406538e-02,\n",
       "        -3.83193390e-02, -2.05314485e-02, -3.60739050e-02, -3.32813766e-02,\n",
       "         4.84228488e-02, -3.42893284e-02,  2.15386879e-03, -7.53746308e-02,\n",
       "        -7.30259097e-02,  2.56577670e-02,  3.47916250e-02,  3.24946707e-02,\n",
       "        -6.93093340e-02, -1.05717292e-01,  1.79637494e-02, -1.22303986e-02,\n",
       "         6.14684014e-02, -3.02703526e-02,  4.96731172e-02, -8.60174056e-02,\n",
       "         4.40218127e-02,  7.31764939e-02, -1.23653780e-01, -4.22080504e-02,\n",
       "        -6.76717111e-02, -1.75514530e-01, -2.86925081e-02,  3.90510070e-02,\n",
       "        -1.25333238e-02,  1.56922885e-02, -9.44472655e-02, -2.38477276e-02,\n",
       "         8.61368598e-02,  6.29688741e-03, -1.52515966e-01, -1.00280258e-01,\n",
       "         5.29894850e-03, -9.08789076e-02, -3.71734807e-02, -3.13913259e-02,\n",
       "         1.52118877e-01,  2.31625438e-01,  3.41522633e-02,  7.00842682e-02,\n",
       "         2.61607835e-02, -9.36003895e-02, -6.12707157e-02, -7.69505464e-03,\n",
       "        -4.19454928e-02,  7.88169103e-02,  1.64961733e-04,  1.68970402e-01,\n",
       "         9.18471739e-02, -5.00479285e-02,  8.89252417e-03, -7.69190560e-02,\n",
       "        -5.51958904e-02,  1.13892889e-01,  9.87767242e-02, -1.59801090e-01,\n",
       "        -5.48162719e-02, -9.99368727e-03,  2.63153603e-02, -4.53921636e-02,\n",
       "         2.78100693e-02,  1.20248024e-01,  5.98350120e-02, -1.10511403e-01,\n",
       "         5.03539108e-03,  8.78649894e-02, -2.98742997e-02, -8.85674693e-02,\n",
       "        -1.75199881e-02,  5.87906875e-02,  5.07042285e-02, -5.56659270e-02,\n",
       "        -2.06382768e-02, -1.26860049e-01, -6.21480490e-02, -2.55084402e-02,\n",
       "         1.01011892e-02,  7.58334082e-02,  2.49842789e-02,  1.18883142e-01,\n",
       "        -4.30709119e-02,  1.05456472e-01,  1.25434542e-01,  7.47676194e-03]),\n",
       " 'Arsen': array([-0.04391515, -0.01621634,  0.14661819, -0.06320297, -0.0306419 ,\n",
       "        -0.06955666,  0.15424553,  0.0150143 , -0.03039334,  0.00569746,\n",
       "         0.04430246,  0.00419745, -0.1062863 ,  0.01433994, -0.06594794,\n",
       "        -0.02688361,  0.04015073, -0.05426281,  0.12118334,  0.08570394,\n",
       "        -0.01125051,  0.00988749, -0.08692569, -0.19923954,  0.01522198,\n",
       "         0.02564466,  0.05370923,  0.03908481, -0.08681972,  0.04935009,\n",
       "        -0.06542646, -0.0786915 ,  0.12955627,  0.15418007, -0.00462445,\n",
       "         0.1046618 , -0.10392344,  0.06583484, -0.02824518, -0.00585674,\n",
       "         0.0262631 ,  0.07069334, -0.08591214,  0.12299437,  0.11882538,\n",
       "         0.01594441,  0.01308152, -0.21483837, -0.07322493,  0.11728512,\n",
       "         0.04477416,  0.10587388, -0.06166842, -0.01862612,  0.05584603,\n",
       "        -0.00537924, -0.03693447, -0.01521326,  0.11887844,  0.07947573,\n",
       "         0.03455585,  0.13635998, -0.05818351, -0.01946118, -0.02423456,\n",
       "         0.00488287,  0.03409238,  0.06718301,  0.00384401, -0.0041847 ,\n",
       "         0.02482173,  0.0728268 , -0.01227393,  0.0017046 , -0.09360217,\n",
       "        -0.06420208, -0.06478314, -0.10069524, -0.07618044, -0.0495833 ,\n",
       "         0.03547107,  0.18379353,  0.04384595,  0.11000731,  0.00475896,\n",
       "        -0.1036915 , -0.01838441, -0.07333912, -0.05276831,  0.06478369,\n",
       "         0.05525781,  0.07413547,  0.01045207, -0.03048278,  0.00691719,\n",
       "        -0.17132138,  0.07428142,  0.23286624,  0.02482332, -0.0776056 ,\n",
       "        -0.04836188, -0.05281424, -0.0042837 ,  0.04990843,  0.07437242,\n",
       "         0.15941546, -0.00058248, -0.18206595, -0.12870384, -0.00555234,\n",
       "        -0.14365064, -0.00408053, -0.03381589, -0.06152325,  0.20513263,\n",
       "        -0.13949055, -0.03458832, -0.08145105, -0.08678652,  0.04593665,\n",
       "        -0.00506981,  0.03410221, -0.0790469 ,  0.03350657, -0.07918358,\n",
       "         0.10808508,  0.08982611, -0.09260517]),\n",
       " 'Elyor': array([-0.09706744, -0.05326696,  0.03382199, -0.0295127 , -0.06443704,\n",
       "        -0.11632641, -0.02904595,  0.08702158, -0.11480326,  0.09938053,\n",
       "         0.11298359, -0.02700012, -0.05656701, -0.04794815,  0.05546815,\n",
       "        -0.06526902,  0.00527468, -0.08681822, -0.00187097,  0.08511482,\n",
       "         0.04470991, -0.04305623,  0.03312258, -0.16643518,  0.01953967,\n",
       "        -0.04253445, -0.11611271, -0.00319819, -0.11253376,  0.06039869,\n",
       "        -0.17841428, -0.08616252, -0.11287211,  0.00978747, -0.08133646,\n",
       "         0.08958263, -0.07832074,  0.03254563, -0.03869394, -0.05215843,\n",
       "         0.04950466, -0.03598474, -0.10870808,  0.0656131 ,  0.04663089,\n",
       "         0.08419541, -0.03020126, -0.05173972, -0.03164072,  0.05227193,\n",
       "        -0.05011986,  0.04289481, -0.04373357, -0.05929477, -0.01561162,\n",
       "         0.04807036, -0.02590316,  0.02402449,  0.13795263, -0.05200958,\n",
       "         0.06485124,  0.11282606, -0.03669638, -0.04289191,  0.03183014,\n",
       "        -0.09233905,  0.05538234,  0.0127548 , -0.04116907,  0.19979814,\n",
       "        -0.111547  ,  0.02078421,  0.05805586, -0.01376753, -0.09955488,\n",
       "        -0.07062199, -0.04153024, -0.13102831, -0.0430423 ,  0.00122089,\n",
       "         0.06954305,  0.27830717,  0.03215276, -0.07992364,  0.02734758,\n",
       "        -0.10592318, -0.01271564, -0.05480717, -0.14743419,  0.00060441,\n",
       "         0.00339446,  0.19624183,  0.09732468, -0.01410151,  0.07652913,\n",
       "        -0.18888657, -0.02169841,  0.12062569,  0.11630231, -0.02381224,\n",
       "        -0.04300099,  0.11186171,  0.01240191,  0.0258429 ,  0.16188061,\n",
       "         0.03194166,  0.11151589, -0.11489086, -0.04954252,  0.08238186,\n",
       "        -0.0213398 ,  0.05236572, -0.01089867, -0.01455713,  0.05908554,\n",
       "        -0.09054131,  0.00240027,  0.00924611, -0.03836293, -0.07111945,\n",
       "         0.08657935, -0.00396591, -0.10796173,  0.05356469,  0.00078499,\n",
       "         0.00224475, -0.01642713, -0.07655956]),\n",
       " 'Muattar': array([-0.03063307,  0.067417  ,  0.12948751, -0.05540979, -0.01295991,\n",
       "        -0.11604422,  0.14909331,  0.06929165, -0.00573489,  0.01466458,\n",
       "        -0.06486842, -0.13369434, -0.07564978,  0.09224083,  0.03380526,\n",
       "        -0.05955203, -0.00711907, -0.10108546,  0.10893445,  0.08984197,\n",
       "        -0.0310163 ,  0.04353491, -0.04594309, -0.15357703, -0.01327373,\n",
       "         0.05351705, -0.01032143,  0.03771097, -0.08386958,  0.17060213,\n",
       "        -0.11873863, -0.0982327 ,  0.02840857,  0.10509761, -0.01136251,\n",
       "         0.10856015, -0.12423613,  0.03145557, -0.00040488,  0.00273064,\n",
       "         0.07767676,  0.08142915, -0.09574465,  0.01926139,  0.07699552,\n",
       "        -0.01249956, -0.00189238, -0.10276124, -0.08632163,  0.12678338,\n",
       "         0.03313984,  0.0665597 , -0.13249336, -0.013531  ,  0.00988685,\n",
       "         0.07344953,  0.02972932, -0.04858094, -0.02178894, -0.06308272,\n",
       "        -0.02507529,  0.07790243, -0.14230636, -0.06994849, -0.07287226,\n",
       "        -0.14104645, -0.04360252,  0.00931453,  0.0380476 ,  0.05536015,\n",
       "        -0.14832163, -0.03110183,  0.11470475,  0.08097744, -0.09644176,\n",
       "        -0.09926998,  0.00753039, -0.1147569 , -0.12678487,  0.00436088,\n",
       "         0.10269822,  0.1267572 ,  0.04247992, -0.01105498,  0.03522633,\n",
       "        -0.09130508,  0.00700757, -0.02402353, -0.06102144,  0.1084235 ,\n",
       "         0.03387633,  0.18691295,  0.09202875, -0.06929883, -0.07801696,\n",
       "        -0.06501915, -0.02205565,  0.08470946,  0.05686093, -0.13488437,\n",
       "         0.01626865, -0.04427264, -0.00545753,  0.04842205, -0.00838613,\n",
       "         0.11510598,  0.0928288 , -0.21083398,  0.0355208 ,  0.0671018 ,\n",
       "         0.00722219, -0.17468698,  0.00301223, -0.02516485, -0.01563729,\n",
       "        -0.09201884,  0.01787876, -0.11239956, -0.12094195, -0.02869227,\n",
       "         0.0283057 ,  0.04188857,  0.05342754,  0.1223976 , -0.0440781 ,\n",
       "         0.05193121,  0.09889577,  0.00673627]),\n",
       " 'Sohib': array([-0.11632025,  0.01152918,  0.12218233,  0.01323955, -0.1461832 ,\n",
       "        -0.0841451 ,  0.12280317,  0.15672435, -0.0338003 ,  0.07081717,\n",
       "         0.02372901,  0.00771068, -0.01634483, -0.01135361, -0.00651919,\n",
       "        -0.10255885,  0.06919885, -0.06363917,  0.08884375, -0.02216558,\n",
       "         0.01361854,  0.02222449,  0.01734033, -0.04740779,  0.00175655,\n",
       "        -0.09227866,  0.02812452,  0.11469345, -0.05301291,  0.06335501,\n",
       "        -0.06938238, -0.09472218, -0.0647746 , -0.02774036,  0.00565423,\n",
       "         0.09840228, -0.03516433, -0.00550983, -0.04117245, -0.01985334,\n",
       "         0.0366538 ,  0.01363223, -0.16611899,  0.00262907,  0.09328969,\n",
       "         0.08272388,  0.05436317, -0.10378976, -0.00229655,  0.05567927,\n",
       "        -0.04300629, -0.03566632, -0.02296399,  0.01126972, -0.08845009,\n",
       "         0.09465341,  0.02095828, -0.00061362,  0.08170749,  0.08417285,\n",
       "         0.02953728,  0.17827193, -0.00962897, -0.11539055, -0.03129469,\n",
       "        -0.05566044, -0.11312332, -0.00862477, -0.13107579,  0.06199156,\n",
       "        -0.04706842,  0.08334082,  0.0531092 , -0.0070097 , -0.12180533,\n",
       "         0.01053651, -0.06262569, -0.00268264, -0.04401077,  0.04240608,\n",
       "         0.09411123,  0.22908179, -0.03854125, -0.00760849,  0.04202693,\n",
       "        -0.07926079, -0.01281531, -0.05543431, -0.11010858,  0.06354199,\n",
       "         0.05428551,  0.18951301,  0.01931027,  0.00350036,  0.0159917 ,\n",
       "        -0.18423992,  0.01866362,  0.14544966,  0.03498042,  0.02627701,\n",
       "        -0.03874051,  0.00163444,  0.0833479 ,  0.00602935,  0.10725367,\n",
       "         0.13846787,  0.11385255, -0.13437717, -0.09333806,  0.06013451,\n",
       "        -0.09280813,  0.00257063, -0.12042247, -0.03297559,  0.10145543,\n",
       "        -0.12386329, -0.03067132,  0.01812014, -0.12068516, -0.13093883,\n",
       "         0.10196503,  0.03328776, -0.04681301,  0.04850242, -0.01823259,\n",
       "         0.14015301,  0.04700509,  0.03995302]),\n",
       " 'Rustam': array([-0.04248394, -0.05681355,  0.01018846, -0.06945069, -0.08047585,\n",
       "        -0.06272371,  0.04312533,  0.0622006 , -0.03789742,  0.19563989,\n",
       "         0.06241234,  0.03757517, -0.1580496 , -0.05176169, -0.03392936,\n",
       "        -0.03080982, -0.06629069, -0.09873557, -0.00815197, -0.02227122,\n",
       "        -0.04675877,  0.02526824,  0.00527696, -0.16086479,  0.04239918,\n",
       "        -0.04060135, -0.04929852,  0.01985369,  0.00574257,  0.18425394,\n",
       "        -0.1363503 , -0.14303709, -0.0240281 ,  0.16936065,  0.02045471,\n",
       "         0.08403448, -0.18729227, -0.01243702, -0.044868  , -0.04358058,\n",
       "         0.02117964,  0.05941782, -0.05304071,  0.016139  , -0.06174534,\n",
       "        -0.05560132,  0.07471564, -0.13255611, -0.08073461,  0.08255644,\n",
       "         0.05791434,  0.04936319, -0.12844024,  0.02777578,  0.05829445,\n",
       "        -0.01229014, -0.06218893,  0.0609881 ,  0.15604308, -0.0557551 ,\n",
       "        -0.07555559, -0.01622385, -0.03033454, -0.06712278, -0.00042536,\n",
       "        -0.11805386,  0.07168165,  0.03732351, -0.11494834, -0.04491909,\n",
       "        -0.07053691, -0.04676277,  0.09331434, -0.01229335,  0.00277198,\n",
       "        -0.08473079,  0.01675857, -0.21700787,  0.06089765, -0.01325544,\n",
       "         0.06797819,  0.2025427 , -0.03920499, -0.00155575, -0.00930869,\n",
       "        -0.19812603,  0.00082915,  0.03074736, -0.09288913, -0.03076896,\n",
       "         0.03816474,  0.22583351,  0.00733694, -0.0789734 , -0.11474206,\n",
       "        -0.10460619, -0.02935272,  0.083073  ,  0.09673948, -0.04323341,\n",
       "        -0.03707577,  0.03568991,  0.05823854,  0.01978186,  0.03618585,\n",
       "         0.12666196,  0.11255412, -0.17960037, -0.11279848,  0.16987651,\n",
       "        -0.0060997 , -0.01528414,  0.02199047, -0.01262079,  0.03521171,\n",
       "        -0.11680666,  0.00440002, -0.05042762, -0.02905833, -0.02991649,\n",
       "         0.13208048,  0.14208514,  0.02216738,  0.02375471, -0.01474583,\n",
       "         0.02567243,  0.11892055, -0.06835893]),\n",
       " 'Sardor': array([-0.21063939, -0.04949094,  0.0741796 , -0.07266235, -0.05894154,\n",
       "        -0.10701991,  0.02074288, -0.04346952, -0.13598648,  0.06964454,\n",
       "         0.06533645, -0.04566201,  0.04155387,  0.01138105,  0.03839875,\n",
       "        -0.00349418, -0.00866196, -0.01611721,  0.04533789,  0.06056676,\n",
       "         0.00363508, -0.10599343,  0.06878503, -0.14748003, -0.02193576,\n",
       "         0.00072485, -0.10931668,  0.10711513, -0.20692164,  0.00367634,\n",
       "        -0.13684984, -0.04624715, -0.10456063,  0.0444714 , -0.16017936,\n",
       "         0.08281757, -0.0795259 ,  0.14259916, -0.04742211, -0.07510704,\n",
       "         0.04629163, -0.01448939, -0.01254605,  0.07514942,  0.0100617 ,\n",
       "         0.00639459, -0.07421953, -0.0596737 ,  0.00750821,  0.08921887,\n",
       "        -0.03570122, -0.07184995,  0.02997596, -0.0408129 ,  0.01733415,\n",
       "         0.05004875,  0.03174153,  0.12676772,  0.12174352, -0.02869141,\n",
       "         0.06353831,  0.12744719, -0.00869549, -0.00833154, -0.05005367,\n",
       "        -0.04442419, -0.00191159,  0.00040764, -0.06615139,  0.18601516,\n",
       "        -0.10830252,  0.04659426,  0.04088517, -0.01067554, -0.12221159,\n",
       "        -0.04569052, -0.05388563, -0.03268219, -0.08466189,  0.07568449,\n",
       "         0.11240505,  0.19432532,  0.07816011,  0.02878204, -0.07506157,\n",
       "        -0.11368937,  0.09125876, -0.06057031, -0.13039144,  0.06159898,\n",
       "         0.01802175,  0.17638325,  0.01931103, -0.02532957, -0.0081873 ,\n",
       "        -0.10038419,  0.02135598,  0.14214149,  0.10607242, -0.00874325,\n",
       "        -0.03545748,  0.13632398,  0.07334447,  0.01587977,  0.14988101,\n",
       "         0.14173757,  0.12038026, -0.0112095 , -0.09848898,  0.05123783,\n",
       "        -0.06690169,  0.10992423,  0.0199418 , -0.02009099,  0.04967257,\n",
       "        -0.1273906 ,  0.07332669,  0.0580897 ,  0.01747073, -0.01326028,\n",
       "         0.08217004,  0.02635927, -0.12919348,  0.10531251,  0.01172082,\n",
       "         0.0573347 , -0.01501659, -0.08221167])}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pipresence.config import Config\n",
    "from pipresence.tools.utils import load_database\n",
    "\n",
    "\n",
    "Config.update_config(\n",
    "    embeddings_file = \"../data/encodings/face_embeddings.pkl\"\n",
    ")\n",
    "database = load_database()\n",
    "\n",
    "database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
