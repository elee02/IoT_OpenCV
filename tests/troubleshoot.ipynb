{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Preprocessing image for face detection\n",
      "[INFO] Running inference on the image\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "session = ort.InferenceSession(\"../data/models/yolov8n.onnx\")\n",
    "input_name = session.get_inputs()[0].name\n",
    "\n",
    "image = cv2.imread(\"../data/test_images/common.jpg\")\n",
    "\n",
    "# Preprocess image for ONNX model\n",
    "print(\"[INFO] Preprocessing image for face detection\")\n",
    "# Resize image to the required size for YOLO model\n",
    "input_image = cv2.resize(image, (640, 640))\n",
    "# Normalize pixel values to range [0, 1]\n",
    "input_image = input_image.astype(np.float32) / 255.0\n",
    "# Change image layout to channel-first format as required by ONNX model\n",
    "input_image = np.transpose(input_image, (2, 0, 1))  # Channel first\n",
    "# Add batch dimension (needed by the model)\n",
    "input_image = np.expand_dims(input_image, axis=0)\n",
    "\n",
    "# Perform inference on the provided image\n",
    "print(\"[INFO] Running inference on the image\")\n",
    "\n",
    "output = session.run(\n",
    "    output_names=None, \n",
    "    input_feed= {input_name: input_image}\n",
    ")\n",
    "outputs = output[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 22.25124 ,  63.748386,  87.80733 , 129.69324 , 174.27881 ,\n",
       "       192.77115 , 213.32562 , 269.3759  , 316.48227 , 322.21863 ,\n",
       "       320.0763  , 316.99564 , 318.22125 , 326.87085 , 433.69553 ,\n",
       "       461.33926 , 492.1537  , 521.9314  , 574.0167  , 586.5244  ,\n",
       "        22.237974,  64.08118 , 112.29887 , 154.78146 , 179.81332 ,\n",
       "       190.1046  , 224.40012 , 279.797   , 318.0047  , 318.1396  ,\n",
       "       320.30872 , 317.26102 , 318.1406  , 335.68372 , 430.21753 ,\n",
       "       459.9945  , 492.00806 , 498.9185  , 567.9201  , 587.4642  ,\n",
       "        22.73684 ,  68.41667 , 181.43848 , 173.88988 , 179.5802  ,\n",
       "       191.43561 , 238.93619 , 288.25153 , 318.90552 , 317.70673 ,\n",
       "       321.46643 , 318.6952  , 319.8886  , 353.45447 , 402.1404  ,\n",
       "       459.18643 , 494.7537  , 490.04715 , 542.8994  , 588.4896  ,\n",
       "        22.670723,  91.90434 , 216.26353 , 197.72499 , 181.47786 ,\n",
       "       196.88611 , 270.8847  , 307.2757  , 320.03256 , 318.68646 ,\n",
       "       323.23904 , 320.78094 , 317.70187 , 334.6641  , 363.4209  ,\n",
       "       436.91595 , 483.63205 , 472.1972  , 524.3074  , 590.328   ,\n",
       "        24.313564, 143.87476 , 197.07613 , 214.93752 , 202.09161 ,\n",
       "       218.15901 , 282.89044 , 311.0667  , 319.37445 , 317.96368 ,\n",
       "       324.19092 , 313.88373 , 314.94144 , 326.45734 , 349.84894 ,\n",
       "       391.12936 , 453.38788 , 472.55856 , 497.5859  , 574.4389  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0][8300:8400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces = []\n",
    "\n",
    "for detection in outputs:\n",
    "    # Extract bounding box coordinates\n",
    "    center_x, center_y, width, height = detection[:4]\n",
    "\n",
    "    # Denormalize to original image size (assuming image size of 640x640)\n",
    "    center_x *= 640\n",
    "    center_y *= 640\n",
    "    width *= 640\n",
    "    height *= 640\n",
    "\n",
    "    # Convert to (x1, y1, x2, y2)\n",
    "    x1 = int(center_x - width / 2)\n",
    "    y1 = int(center_y - height / 2)\n",
    "    x2 = int(center_x + width / 2)\n",
    "    y2 = int(center_y + height / 2)\n",
    "\n",
    "    # Extract objectness score\n",
    "    objectness_score = detection[4]\n",
    "    confidence_threshold = 0.65\n",
    "    if objectness_score < confidence_threshold:\n",
    "        continue\n",
    "\n",
    "    # Extract class scores and determine the class with highest confidence\n",
    "    class_scores = detection[5:]\n",
    "    class_id = np.argmax(class_scores)\n",
    "    class_confidence = class_scores[class_id]\n",
    "\n",
    "    # Set a class confidence threshold\n",
    "    if class_confidence > 0.5:\n",
    "        faces.append([x1, y1, x2, y2, class_id, class_confidence])\n",
    "        # Optionally draw the bounding box on the image\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), (255, 0, 0), 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_OpNamespace' 'torchvision' object has no attribute 'nms'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 57\u001b[0m\n\u001b[1;32m     55\u001b[0m boxes_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(boxes)\n\u001b[1;32m     56\u001b[0m confidences_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(confidences)\n\u001b[0;32m---> 57\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnms\u001b[49m(boxes_tensor, confidences_tensor, iou_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m     58\u001b[0m boxes \u001b[38;5;241m=\u001b[39m boxes[indices]\n\u001b[1;32m     59\u001b[0m confidences \u001b[38;5;241m=\u001b[39m confidences[indices]\n",
      "File \u001b[0;32m~/PiPresence/.venv/lib/python3.10/site-packages/torch/_ops.py:1225\u001b[0m, in \u001b[0;36m_OpNamespace.__getattr__\u001b[0;34m(self, op_name)\u001b[0m\n\u001b[1;32m   1223\u001b[0m     op, overload_names \u001b[38;5;241m=\u001b[39m _get_packet(qualified_op_name, module_name)\n\u001b[1;32m   1224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m op \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1225\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1226\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_OpNamespace\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mop_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1227\u001b[0m         )\n\u001b[1;32m   1228\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1229\u001b[0m     \u001b[38;5;66;03m# Turn this into AttributeError so getattr(obj, key, default)\u001b[39;00m\n\u001b[1;32m   1230\u001b[0m     \u001b[38;5;66;03m# works (this is called by TorchScript with __origin__)\u001b[39;00m\n\u001b[1;32m   1231\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1232\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_OpNamespace\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mop_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1233\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_OpNamespace' 'torchvision' object has no attribute 'nms'"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Load the model\n",
    "session = ort.InferenceSession(\"../data/models/yolov8n.onnx\")\n",
    "input_name = session.get_inputs()[0].name\n",
    "\n",
    "# Load and preprocess the image\n",
    "image = cv2.imread(\"../data/test_images/common.jpg\")\n",
    "input_image = cv2.resize(image, (640, 640))\n",
    "input_image = input_image.astype(np.float32) / 255.0\n",
    "input_image = np.transpose(input_image, (2, 0, 1))  # Channel first\n",
    "input_image = np.expand_dims(input_image, axis=0)\n",
    "\n",
    "# Run inference\n",
    "output = session.run(None, {input_name: input_image})\n",
    "\n",
    "# Process the outputs\n",
    "outputs = output[0]\n",
    "outputs = outputs.transpose(0, 2, 1)\n",
    "outputs = outputs[0]\n",
    "\n",
    "# Extract boxes and scores\n",
    "boxes = outputs[:, :4]\n",
    "scores = outputs[:, 4:]\n",
    "\n",
    "# Apply activation functions\n",
    "class_probs = 1 / (1 + np.exp(-scores))\n",
    "\n",
    "# Decode bounding boxes\n",
    "boxes[:, 0] = boxes[:, 0] - boxes[:, 2] / 2  # x1\n",
    "boxes[:, 1] = boxes[:, 1] - boxes[:, 3] / 2  # y1\n",
    "boxes[:, 2] = boxes[:, 0] + boxes[:, 2]      # x2\n",
    "boxes[:, 3] = boxes[:, 1] + boxes[:, 3]      # y2\n",
    "\n",
    "# Filter predictions\n",
    "confidences = np.max(class_probs, axis=1)\n",
    "class_ids = np.argmax(class_probs, axis=1)\n",
    "conf_threshold = 0.5\n",
    "mask = confidences > conf_threshold\n",
    "boxes = boxes[mask]\n",
    "confidences = confidences[mask]\n",
    "class_ids = class_ids[mask]\n",
    "\n",
    "# Rescale boxes\n",
    "orig_height, orig_width = image.shape[:2]\n",
    "scale_x = orig_width / 640\n",
    "scale_y = orig_height / 640\n",
    "boxes[:, [0, 2]] *= scale_x\n",
    "boxes[:, [1, 3]] *= scale_y\n",
    "\n",
    "# Apply NMS\n",
    "boxes_tensor = torch.tensor(boxes)\n",
    "confidences_tensor = torch.tensor(confidences)\n",
    "indices = torch.ops.torchvision.nms(boxes_tensor, confidences_tensor, iou_threshold=0.5)\n",
    "boxes = boxes[indices]\n",
    "confidences = confidences[indices]\n",
    "class_ids = class_ids[indices]\n",
    "\n",
    "# Draw boxes on the image\n",
    "for box, conf, class_id in zip(boxes, confidences, class_ids):\n",
    "    x1, y1, x2, y2 = box.astype(int)\n",
    "    label = f\"Class {class_id}: {conf:.2f}\"\n",
    "    cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "    cv2.putText(image, label, (x1, y1 - 10),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "# Display the image\n",
    "cv2.imshow(\"Detections\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ../data/models/yolov8n.onnx for ONNX Runtime inference...\n",
      "WARNING ⚠️ Failed to start ONNX Runtime session with CUDA. Falling back to CPU...\n",
      "Preferring ONNX Runtime AzureExecutionProvider\n",
      "\n",
      "image 1/1 /home/el02/PiPresence/tests/../data/known_faces/elyor/front.jpg: 640x640 1 person, 33.7ms\n",
      "Speed: 0.7ms preprocess, 33.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"../data/models/yolov8n.onnx\", task=\"detect\")\n",
    "results = model(\"../data/known_faces/elyor/front.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer names:\n",
      "['', 'model', 'model.0', 'model.0.conv', 'model.0.bn', 'model.0.act', 'model.1', 'model.1.conv', 'model.1.bn', 'model.2', 'model.2.cv1', 'model.2.cv1.conv', 'model.2.cv1.bn', 'model.2.cv2', 'model.2.cv2.conv', 'model.2.cv2.bn', 'model.2.m', 'model.2.m.0', 'model.2.m.0.cv1', 'model.2.m.0.cv1.conv', 'model.2.m.0.cv1.bn', 'model.2.m.0.cv2', 'model.2.m.0.cv2.conv', 'model.2.m.0.cv2.bn', 'model.3', 'model.3.conv', 'model.3.bn', 'model.4', 'model.4.cv1', 'model.4.cv1.conv', 'model.4.cv1.bn', 'model.4.cv2', 'model.4.cv2.conv', 'model.4.cv2.bn', 'model.4.m', 'model.4.m.0', 'model.4.m.0.cv1', 'model.4.m.0.cv1.conv', 'model.4.m.0.cv1.bn', 'model.4.m.0.cv2', 'model.4.m.0.cv2.conv', 'model.4.m.0.cv2.bn', 'model.4.m.1', 'model.4.m.1.cv1', 'model.4.m.1.cv1.conv', 'model.4.m.1.cv1.bn', 'model.4.m.1.cv2', 'model.4.m.1.cv2.conv', 'model.4.m.1.cv2.bn', 'model.5', 'model.5.conv', 'model.5.bn', 'model.6', 'model.6.cv1', 'model.6.cv1.conv', 'model.6.cv1.bn', 'model.6.cv2', 'model.6.cv2.conv', 'model.6.cv2.bn', 'model.6.m', 'model.6.m.0', 'model.6.m.0.cv1', 'model.6.m.0.cv1.conv', 'model.6.m.0.cv1.bn', 'model.6.m.0.cv2', 'model.6.m.0.cv2.conv', 'model.6.m.0.cv2.bn', 'model.6.m.1', 'model.6.m.1.cv1', 'model.6.m.1.cv1.conv', 'model.6.m.1.cv1.bn', 'model.6.m.1.cv2', 'model.6.m.1.cv2.conv', 'model.6.m.1.cv2.bn', 'model.7', 'model.7.conv', 'model.7.bn', 'model.8', 'model.8.cv1', 'model.8.cv1.conv', 'model.8.cv1.bn', 'model.8.cv2', 'model.8.cv2.conv', 'model.8.cv2.bn', 'model.8.m', 'model.8.m.0', 'model.8.m.0.cv1', 'model.8.m.0.cv1.conv', 'model.8.m.0.cv1.bn', 'model.8.m.0.cv2', 'model.8.m.0.cv2.conv', 'model.8.m.0.cv2.bn', 'model.9', 'model.9.cv1', 'model.9.cv1.conv', 'model.9.cv1.bn', 'model.9.cv2', 'model.9.cv2.conv', 'model.9.cv2.bn', 'model.9.m', 'model.10', 'model.11', 'model.12', 'model.12.cv1', 'model.12.cv1.conv', 'model.12.cv1.bn', 'model.12.cv2', 'model.12.cv2.conv', 'model.12.cv2.bn', 'model.12.m', 'model.12.m.0', 'model.12.m.0.cv1', 'model.12.m.0.cv1.conv', 'model.12.m.0.cv1.bn', 'model.12.m.0.cv2', 'model.12.m.0.cv2.conv', 'model.12.m.0.cv2.bn', 'model.13', 'model.14', 'model.15', 'model.15.cv1', 'model.15.cv1.conv', 'model.15.cv1.bn', 'model.15.cv2', 'model.15.cv2.conv', 'model.15.cv2.bn', 'model.15.m', 'model.15.m.0', 'model.15.m.0.cv1', 'model.15.m.0.cv1.conv', 'model.15.m.0.cv1.bn', 'model.15.m.0.cv2', 'model.15.m.0.cv2.conv', 'model.15.m.0.cv2.bn', 'model.16', 'model.16.conv', 'model.16.bn', 'model.17', 'model.18', 'model.18.cv1', 'model.18.cv1.conv', 'model.18.cv1.bn', 'model.18.cv2', 'model.18.cv2.conv', 'model.18.cv2.bn', 'model.18.m', 'model.18.m.0', 'model.18.m.0.cv1', 'model.18.m.0.cv1.conv', 'model.18.m.0.cv1.bn', 'model.18.m.0.cv2', 'model.18.m.0.cv2.conv', 'model.18.m.0.cv2.bn', 'model.19', 'model.19.conv', 'model.19.bn', 'model.20', 'model.21', 'model.21.cv1', 'model.21.cv1.conv', 'model.21.cv1.bn', 'model.21.cv2', 'model.21.cv2.conv', 'model.21.cv2.bn', 'model.21.m', 'model.21.m.0', 'model.21.m.0.cv1', 'model.21.m.0.cv1.conv', 'model.21.m.0.cv1.bn', 'model.21.m.0.cv2', 'model.21.m.0.cv2.conv', 'model.21.m.0.cv2.bn', 'model.22', 'model.22.cv2', 'model.22.cv2.0', 'model.22.cv2.0.0', 'model.22.cv2.0.0.conv', 'model.22.cv2.0.0.bn', 'model.22.cv2.0.1', 'model.22.cv2.0.1.conv', 'model.22.cv2.0.1.bn', 'model.22.cv2.0.2', 'model.22.cv2.1', 'model.22.cv2.1.0', 'model.22.cv2.1.0.conv', 'model.22.cv2.1.0.bn', 'model.22.cv2.1.1', 'model.22.cv2.1.1.conv', 'model.22.cv2.1.1.bn', 'model.22.cv2.1.2', 'model.22.cv2.2', 'model.22.cv2.2.0', 'model.22.cv2.2.0.conv', 'model.22.cv2.2.0.bn', 'model.22.cv2.2.1', 'model.22.cv2.2.1.conv', 'model.22.cv2.2.1.bn', 'model.22.cv2.2.2', 'model.22.cv3', 'model.22.cv3.0', 'model.22.cv3.0.0', 'model.22.cv3.0.0.conv', 'model.22.cv3.0.0.bn', 'model.22.cv3.0.1', 'model.22.cv3.0.1.conv', 'model.22.cv3.0.1.bn', 'model.22.cv3.0.2', 'model.22.cv3.1', 'model.22.cv3.1.0', 'model.22.cv3.1.0.conv', 'model.22.cv3.1.0.bn', 'model.22.cv3.1.1', 'model.22.cv3.1.1.conv', 'model.22.cv3.1.1.bn', 'model.22.cv3.1.2', 'model.22.cv3.2', 'model.22.cv3.2.0', 'model.22.cv3.2.0.conv', 'model.22.cv3.2.0.bn', 'model.22.cv3.2.1', 'model.22.cv3.2.1.conv', 'model.22.cv3.2.1.bn', 'model.22.cv3.2.2', 'model.22.dfl', 'model.22.dfl.conv']\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the model\n",
    "model = YOLO('../data/models/yolov8n.pt')\n",
    "\n",
    "# Get the model architecture (Pytorch model)\n",
    "yolo_model = model.model\n",
    "\n",
    "# Get the names of all layers\n",
    "layer_names = [name for name, _ in yolo_model.named_modules()]\n",
    "\n",
    "print(\"Layer names:\")\n",
    "print(layer_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/el02/PiPresence/.venv/lib/python3.10/site-packages/ultralytics/nn/modules/head.py:105: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if self.format != \"imx\" and (self.dynamic or self.shape != shape):\n",
      "/home/el02/PiPresence/.venv/lib/python3.10/site-packages/ultralytics/utils/tal.py:308: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n",
      "  for i, stride in enumerate(strides):\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define a dummy input\n",
    "dummy_input = torch.randn(1, 3, 640, 640)\n",
    "\n",
    "# Export to ONNX\n",
    "torch.onnx.export(yolo_model, dummy_input, \"data/models/yolov8n.onnx\",\n",
    "                  input_names=['input'],\n",
    "                  output_names=['output_boxes', 'output_classes'],\n",
    "                  dynamic_axes={'input': {0: 'batch_size'}, 'output_boxes': {0: 'batch_size'}, 'output_classes': {0: 'batch_size'}})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Preprocessing image for face detection\n",
      "[INFO] Running inference on the image\n",
      "Output shape: (1, 5, 8400)\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the ONNX model\n",
    "session = ort.InferenceSession(\"../data/models/yolov8n-face.onnx\")\n",
    "\n",
    "# Get the input name for ONNX model\n",
    "input_name = session.get_inputs()[0].name\n",
    "\n",
    "# Get the output names from ONNX model\n",
    "output_names = [output.name for output in session.get_outputs()]\n",
    "\n",
    "# Load and preprocess the image\n",
    "image = cv2.imread(\"../data/test_images/friends_gathering.jpg\")\n",
    "print(\"[INFO] Preprocessing image for face detection\")\n",
    "input_image = cv2.resize(image, (640, 640))\n",
    "input_image = input_image.astype(np.float32) / 255.0\n",
    "input_image = np.transpose(input_image, (2, 0, 1))  # Channel first\n",
    "input_image = np.expand_dims(input_image, axis=0)\n",
    "\n",
    "# Perform inference on the provided image\n",
    "print(\"[INFO] Running inference on the image\")\n",
    "outputs = session.run(output_names, {input_name: input_image})\n",
    "\n",
    "# Output shape is likely (1, 84, 8400)\n",
    "print(\"Output shape:\", outputs[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ultralytics YOLO 🚀, AGPL-3.0 license\n",
    "\n",
    "import argparse\n",
    "\n",
    "import cv2.dnn\n",
    "import numpy as np\n",
    "\n",
    "from ultralytics.utils import ASSETS, yaml_load\n",
    "from ultralytics.utils.checks import check_yaml\n",
    "\n",
    "CLASSES = yaml_load(check_yaml(\"coco8.yaml\"))[\"names\"]\n",
    "colors = np.random.uniform(0, 255, size=(len(CLASSES), 3))\n",
    "\n",
    "\n",
    "def draw_bounding_box(img, class_id, confidence, x, y, x_plus_w, y_plus_h):\n",
    "    \"\"\"\n",
    "    Draws bounding boxes on the input image based on the provided arguments.\n",
    "\n",
    "    Args:\n",
    "        img (numpy.ndarray): The input image to draw the bounding box on.\n",
    "        class_id (int): Class ID of the detected object.\n",
    "        confidence (float): Confidence score of the detected object.\n",
    "        x (int): X-coordinate of the top-left corner of the bounding box.\n",
    "        y (int): Y-coordinate of the top-left corner of the bounding box.\n",
    "        x_plus_w (int): X-coordinate of the bottom-right corner of the bounding box.\n",
    "        y_plus_h (int): Y-coordinate of the bottom-right corner of the bounding box.\n",
    "    \"\"\"\n",
    "    label = f\"{CLASSES[class_id]} ({confidence:.2f})\"\n",
    "    color = colors[class_id]\n",
    "    cv2.rectangle(img, (x, y), (x_plus_w, y_plus_h), color, 2)\n",
    "    cv2.putText(img, label, (x - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "\n",
    "def main(onnx_model, input_image):\n",
    "    \"\"\"\n",
    "    Main function to load ONNX model, perform inference, draw bounding boxes, and display the output image.\n",
    "\n",
    "    Args:\n",
    "        onnx_model (str): Path to the ONNX model.\n",
    "        input_image (str): Path to the input image.\n",
    "\n",
    "    Returns:\n",
    "        list: List of dictionaries containing detection information such as class_id, class_name, confidence, etc.\n",
    "    \"\"\"\n",
    "    # Load the ONNX model\n",
    "    model: cv2.dnn.Net = cv2.dnn.readNetFromONNX(onnx_model)\n",
    "\n",
    "    # Read the input image\n",
    "    original_image: np.ndarray = cv2.imread(input_image)\n",
    "    [height, width, _] = original_image.shape\n",
    "\n",
    "    # Prepare a square image for inference\n",
    "    length = max((height, width))\n",
    "    image = np.zeros((length, length, 3), np.uint8)\n",
    "    image[0:height, 0:width] = original_image\n",
    "\n",
    "    # Calculate scale factor\n",
    "    scale = length / 640\n",
    "\n",
    "    # Preprocess the image and prepare blob for model\n",
    "    blob = cv2.dnn.blobFromImage(image, scalefactor=1 / 255, size=(640, 640), swapRB=True)\n",
    "    model.setInput(blob)\n",
    "\n",
    "    # Perform inference\n",
    "    outputs = model.forward()\n",
    "\n",
    "    # Prepare output array\n",
    "    outputs = np.array([cv2.transpose(outputs[0])])\n",
    "    rows = outputs.shape[1]\n",
    "\n",
    "    boxes = []\n",
    "    scores = []\n",
    "    class_ids = []\n",
    "\n",
    "    # Iterate through output to collect bounding boxes, confidence scores, and class IDs\n",
    "    for i in range(rows):\n",
    "        classes_scores = outputs[0][i][4:]\n",
    "        (minScore, maxScore, minClassLoc, (x, maxClassIndex)) = cv2.minMaxLoc(classes_scores)\n",
    "        if maxScore >= 0.25:\n",
    "            box = [\n",
    "                outputs[0][i][0] - (0.5 * outputs[0][i][2]),\n",
    "                outputs[0][i][1] - (0.5 * outputs[0][i][3]),\n",
    "                outputs[0][i][2],\n",
    "                outputs[0][i][3],\n",
    "            ]\n",
    "            boxes.append(box)\n",
    "            scores.append(maxScore)\n",
    "            class_ids.append(maxClassIndex)\n",
    "\n",
    "    # Apply NMS (Non-maximum suppression)\n",
    "    result_boxes = cv2.dnn.NMSBoxes(boxes, scores, 0.25, 0.45, 0.5)\n",
    "\n",
    "    detections = []\n",
    "\n",
    "    # Iterate through NMS results to draw bounding boxes and labels\n",
    "    for i in range(len(result_boxes)):\n",
    "        index = result_boxes[i]\n",
    "        box = boxes[index]\n",
    "        detection = {\n",
    "            \"class_id\": class_ids[index],\n",
    "            \"class_name\": CLASSES[class_ids[index]],\n",
    "            \"confidence\": scores[index],\n",
    "            \"box\": box,\n",
    "            \"scale\": scale,\n",
    "        }\n",
    "        detections.append(detection)\n",
    "        draw_bounding_box(\n",
    "            original_image,\n",
    "            class_ids[index],\n",
    "            scores[index],\n",
    "            round(box[0] * scale),\n",
    "            round(box[1] * scale),\n",
    "            round((box[0] + box[2]) * scale),\n",
    "            round((box[1] + box[3]) * scale),\n",
    "        )\n",
    "\n",
    "    # Display the image with bounding boxes\n",
    "    cv2.imshow(\"image\", original_image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'class_id': 0,\n",
       "  'class_name': 'person',\n",
       "  'confidence': 0.9291252493858337,\n",
       "  'box': [np.float32(72.9223),\n",
       "   np.float32(2.3677368),\n",
       "   np.float32(496.26514),\n",
       "   np.float32(353.2938)],\n",
       "  'scale': 1.875}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main(\"../data/models/yolov8n.onnx\", \"../data/test_images/common.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading YOLOv8n face-model from ../data/models/yolov8n-face.onnx\n"
     ]
    }
   ],
   "source": [
    "from pipresence.detect_faces import FaceDetector\n",
    "import cv2 \n",
    "\n",
    "detector = FaceDetector(\"../data/models/yolov8n-face.onnx\")\n",
    "image = cv2.imread(\"../data/known_faces/elyor/right.jpg\")\n",
    "detections = detector.detect_faces(image)\n",
    "detection = detections[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'class_id': 0, 'class_name': 'person', 'confidence': 0.3326736390590668, 'box': [np.float32(195.62439), np.float32(135.87161), np.float32(293.73694), np.float32(217.464)], 'scale': 1.0}]\n",
      "[INFO] 'q' pressed, exiting the application\n"
     ]
    }
   ],
   "source": [
    "print(detections)\n",
    "bbox = detection[\"box\"]\n",
    "x = round(bbox[0] * detection[\"scale\"])\n",
    "y = round(bbox[1] * detection[\"scale\"])\n",
    "x_plus_w = round((bbox[0] + bbox[2]) * detection[\"scale\"])\n",
    "y_plus_h = round((bbox[1] + bbox[3]) * detection[\"scale\"])\n",
    "color = (200, 56, 159)\n",
    "cv2.rectangle(image, (x, y), (x_plus_w, y_plus_h), color, 2)\n",
    "cv2.putText(image, \"Common\", (x - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "while True:\n",
    "    cv2.imshow(\"Example\", image)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        print(\"[INFO] 'q' pressed, exiting the application\")\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading MobileFaceNet model from ../data/models/mobilefacenet_fixed.onnx\n"
     ]
    },
    {
     "ename": "InvalidArgument",
     "evalue": "[ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Got invalid dimensions for input: input:0 for the following indices\n index: 1 Got: 3 Expected: 112\n index: 2 Got: 640 Expected: 112\n index: 3 Got: 640 Expected: 3\n Please fix either the inputs/outputs or the model.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgument\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m preprocessed_image \u001b[38;5;241m=\u001b[39m recognizer\u001b[38;5;241m.\u001b[39mpreprocess(image)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Run inference\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mrecognizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mrecognizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocessed_image\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PiPresence/.venv/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:266\u001b[0m, in \u001b[0;36mSession.run\u001b[0;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[1;32m    264\u001b[0m     output_names \u001b[38;5;241m=\u001b[39m [output\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs_meta]\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_feed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m C\u001b[38;5;241m.\u001b[39mEPFail \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_fallback:\n",
      "\u001b[0;31mInvalidArgument\u001b[0m: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Got invalid dimensions for input: input:0 for the following indices\n index: 1 Got: 3 Expected: 112\n index: 2 Got: 640 Expected: 112\n index: 3 Got: 640 Expected: 3\n Please fix either the inputs/outputs or the model."
     ]
    }
   ],
   "source": [
    "from pipresence.recognize_faces import FaceRecognizer\n",
    "from pipresence.config import Config \n",
    "import cv2\n",
    "\n",
    "Config.update_config(mobilefacenet_model_path=\"../data/models/mobilefacenet_fixed.onnx\")\n",
    "recognizer = FaceRecognizer()\n",
    "image = cv2.imread(\"../data/known_faces/tom/front.jpg\")\n",
    "preprocessed_image = recognizer.preprocess(image)\n",
    "\n",
    "# Run inference\n",
    "outputs = recognizer.session.run(None, {recognizer.input_name: preprocessed_image})\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading MobileFaceNet model from ../data/models/mobilefacenet0.onnx\n",
      "[ERROR] Face recognition failed: 'dict' object has no attribute 'shape'\n"
     ]
    }
   ],
   "source": [
    "from pipresence.recognize_faces import FaceRecognizer\n",
    "import cv2\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "recognizer = FaceRecognizer(\"../data/models/mobilefacenet0.onnx\")\n",
    "embeddings_file = \"../data/encodings/face_embeddings.pkl\"\n",
    "image = cv2.imread(\"../data/known_faces/elyor/right.jpg\")\n",
    "embedding = recognizer.recognize_face(detections[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading known face embeddings from ../data/encodings/face_embeddings.pkl\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "if os.path.exists(embeddings_file):\n",
    "    # Load existing embeddings from the file\n",
    "    print(f\"[INFO] Loading known face embeddings from {embeddings_file}\")\n",
    "    with open(embeddings_file, 'rb') as f:\n",
    "        database = pickle.load(f)\n",
    "\n",
    "# Compare detected face with known faces in the database\n",
    "for name, known_embedding in database.items():\n",
    "    if recognizer.compare_embeddings(embedding, known_embedding):\n",
    "        print(f\"[INFO] Recognized {name}\")\n",
    "        # Annotate the recognized face in the video feed\n",
    "        cv2.putText(image, f\"{name}\", (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        # Display the video feed with annotations\n",
    "\n",
    "cv2.imshow('PiPresence - Attendance Recognition', image)\n",
    "# Exit loop if 'q' is pressed\n",
    "if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "    print(\"[INFO] 'q' pressed, exiting the application\")\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading YOLOv8n face-model from ../data/models/yolov8n-face.onnx\n",
      "[INFO] Loading MobileFaceNet model from ../data/models/mobilefacenet_fixed.onnx\n",
      "[INFO] Processing ../data/images/tom/left.jpg\n",
      "[ERROR] At least one dimension is smaller than 640\n",
      "[ERROR] Failed to process ../data/images/tom/left.jpg\n",
      "[INFO] Processing ../data/images/tom/front.jpg\n",
      "[ERROR] Face recognition failed: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Got invalid dimensions for input: input:0 for the following indices\n",
      " index: 1 Got: 3 Expected: 112\n",
      " index: 3 Got: 112 Expected: 3\n",
      " Please fix either the inputs/outputs or the model.\n",
      "[INFO] Saved processed face to ../data/known_faces/tom/front.jpg\n",
      "[INFO] Processing ../data/images/tom/right.jpg\n",
      "[ERROR] Face recognition failed: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Got invalid dimensions for input: input:0 for the following indices\n",
      " index: 1 Got: 3 Expected: 112\n",
      " index: 3 Got: 112 Expected: 3\n",
      " Please fix either the inputs/outputs or the model.\n",
      "[INFO] Saved processed face to ../data/known_faces/tom/right.jpg\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'NoneType' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 10\u001b[0m\n\u001b[1;32m      3\u001b[0m Config\u001b[38;5;241m.\u001b[39mupdate_config(\n\u001b[1;32m      4\u001b[0m     yolo_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/models/yolov8n-face.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     mobilefacenet_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/models/mobilefacenet_fixed.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     input_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/images/\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m     output_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/known_faces\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m processor \u001b[38;5;241m=\u001b[39m ImagePreprocessor()\n\u001b[0;32m---> 10\u001b[0m \u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_database_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PiPresence/pipresence/preprocess.py:106\u001b[0m, in \u001b[0;36mImagePreprocessor.process_database_images\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m         error_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m embeddings:\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m# Average the embeddings from different profiles to get a more robust representation\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     average_embedding \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     database[person_name] \u001b[38;5;241m=\u001b[39m average_embedding\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INFO] Added \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mperson_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to the known faces database\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/PiPresence/.venv/lib/python3.10/site-packages/numpy/_core/fromnumeric.py:3596\u001b[0m, in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m   3593\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3594\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m mean(axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 3596\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_methods\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mean\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3597\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PiPresence/.venv/lib/python3.10/site-packages/numpy/_core/_methods.py:127\u001b[0m, in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m    124\u001b[0m         dtype \u001b[38;5;241m=\u001b[39m mu\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf4\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    125\u001b[0m         is_float16_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43mumr_sum\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, mu\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _no_nep50_warning():\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'NoneType' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "from pipresence.preprocess import ImagePreprocessor\n",
    "from pipresence.config import Config\n",
    "Config.update_config(\n",
    "    yolo_model_path = \"../data/models/yolov8n-face.onnx\",\n",
    "    mobilefacenet_model_path = \"../data/models/mobilefacenet_fixed.onnx\",\n",
    "    input_directory = \"../data/images/\",\n",
    "    output_directory = \"../data/known_faces\"\n",
    ")\n",
    "processor = ImagePreprocessor()\n",
    "processor.process_database_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'you': 'you', 'me': 'me', 'he': 'he'}\n",
      "(['you', 'me', 'she'], {1: '1', 2: '2', 3: '3'})\n"
     ]
    }
   ],
   "source": [
    "def func(**kwargs):\n",
    "    print(kwargs)\n",
    "\n",
    "func(you=\"you\", me=\"me\", he=\"he\")\n",
    "\n",
    "def func1(*args):\n",
    "    print(args)\n",
    "\n",
    "func1([\"you\", \"me\", \"she\"], {1:\"1\", 2: \"2\", 3: \"3\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eyes are cute\n",
      "Hair is long\n",
      "Height is 176\n",
      "{'eyes': 'sharp', 'hair': 'short', 'height': 173}\n",
      "dict_items([('eyes', 'sharp'), ('hair', 'short'), ('height', 173)])\n",
      "Eyes are sharp\n",
      "Hair is short\n",
      "Height is 173\n"
     ]
    }
   ],
   "source": [
    "class Person:\n",
    "    eyes = \"cute\"\n",
    "    hair = \"long\"\n",
    "    height = 176\n",
    "\n",
    "    @classmethod\n",
    "    def update_vars(cls, **kwargs):\n",
    "        print(kwargs)\n",
    "        print(kwargs.items())\n",
    "        for key, value in kwargs.items():\n",
    "            if hasattr(cls, key):\n",
    "                setattr(cls, key, value)\n",
    "            else:\n",
    "                print(f\"No such variable as {key}\")\n",
    "\n",
    "    @classmethod\n",
    "    def display_vars(cls):\n",
    "        print(f\"Eyes are {cls.eyes}\")\n",
    "        print(f\"Hair is {cls.hair}\")\n",
    "        print(f\"Height is {cls.height}\")\n",
    "\n",
    "Person.display_vars()\n",
    "Person.update_vars(**{\n",
    "        \"eyes\": \"sharp\",\n",
    "        \"hair\": \"short\",\n",
    "        \"height\": 173\n",
    "    }\n",
    ")\n",
    "Person.display_vars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Abubakr': array([-0.1978442 , -0.00302834, -0.02722897, -0.05801077, -0.10105256,\n",
       "        -0.09672569,  0.0895449 ,  0.10473246, -0.04211131,  0.12898778,\n",
       "         0.07911805,  0.03632414, -0.1094389 ,  0.08779069,  0.00898743,\n",
       "        -0.0515371 , -0.04161552, -0.10134095,  0.04073952,  0.03972579,\n",
       "        -0.02362769, -0.09986002, -0.003306  , -0.07574529, -0.03082444,\n",
       "        -0.0120294 ,  0.00125861,  0.02025795, -0.07998296,  0.15857613,\n",
       "        -0.1247013 , -0.07703517, -0.00512416,  0.08261013, -0.04272592,\n",
       "         0.17545326, -0.14579588,  0.08747221, -0.02416261, -0.05242952,\n",
       "         0.06600432,  0.02335808, -0.09444122, -0.00544801,  0.01178771,\n",
       "        -0.05421623, -0.06772334, -0.11302577,  0.01366547,  0.06926548,\n",
       "        -0.01171857, -0.03055242, -0.05336758, -0.00622749,  0.01451243,\n",
       "         0.02659509, -0.03857574,  0.0306123 ,  0.15025533, -0.07283626,\n",
       "        -0.04514927,  0.01894576, -0.07179352, -0.09156752, -0.04327691,\n",
       "        -0.12064178,  0.02731288, -0.01485442, -0.02420474,  0.00036901,\n",
       "        -0.07941285,  0.01689504,  0.0319164 ,  0.03389412, -0.14403082,\n",
       "        -0.08293398,  0.01101746, -0.11071391, -0.05793302,  0.05202798,\n",
       "         0.09930077,  0.29291049, -0.01042145,  0.02158281, -0.01771854,\n",
       "        -0.12263448,  0.0290726 ,  0.00806593, -0.07553181, -0.01131507,\n",
       "         0.04747841,  0.17801851,  0.07096313, -0.07444475, -0.00455205,\n",
       "        -0.04677682, -0.02684394,  0.14125885,  0.09267182,  0.00787985,\n",
       "        -0.05127251,  0.04018702,  0.06549724, -0.00970694,  0.02931299,\n",
       "         0.07038862,  0.06024662, -0.11970024, -0.09692968,  0.08958937,\n",
       "        -0.07848843, -0.02708351, -0.08388056, -0.00888557,  0.06722626,\n",
       "        -0.14196287, -0.008278  , -0.06060724, -0.02177376, -0.11228941,\n",
       "         0.20014296,  0.09942425, -0.06176443,  0.03836172, -0.0907513 ,\n",
       "         0.04628272,  0.03667031, -0.12603855]),\n",
       " 'Arsen': array([-0.04391515, -0.01621634,  0.14661821, -0.06320297, -0.0306419 ,\n",
       "        -0.06955667,  0.15424553,  0.0150143 , -0.03039334,  0.00569746,\n",
       "         0.04430246,  0.00419745, -0.10628629,  0.01433993, -0.06594794,\n",
       "        -0.02688361,  0.04015073, -0.05426281,  0.12118335,  0.08570396,\n",
       "        -0.01125051,  0.00988749, -0.0869257 , -0.19923954,  0.01522198,\n",
       "         0.02564467,  0.05370922,  0.03908481, -0.08681973,  0.04935009,\n",
       "        -0.06542648, -0.0786915 ,  0.12955625,  0.15418006, -0.00462446,\n",
       "         0.1046618 , -0.10392343,  0.06583484, -0.02824518, -0.00585673,\n",
       "         0.02626311,  0.07069335, -0.08591215,  0.12299439,  0.11882539,\n",
       "         0.01594441,  0.01308151, -0.21483836, -0.07322492,  0.11728512,\n",
       "         0.04477416,  0.10587388, -0.0616684 , -0.01862612,  0.05584601,\n",
       "        -0.00537923, -0.03693446, -0.01521325,  0.11887843,  0.07947573,\n",
       "         0.03455584,  0.13635998, -0.05818351, -0.01946118, -0.02423454,\n",
       "         0.00488287,  0.03409238,  0.06718301,  0.00384401, -0.00418469,\n",
       "         0.02482175,  0.07282681, -0.01227395,  0.0017046 , -0.09360217,\n",
       "        -0.06420208, -0.06478314, -0.10069523, -0.07618044, -0.0495833 ,\n",
       "         0.03547107,  0.18379354,  0.04384595,  0.1100073 ,  0.00475896,\n",
       "        -0.10369149, -0.01838442, -0.07333912, -0.05276831,  0.06478369,\n",
       "         0.05525781,  0.07413548,  0.01045207, -0.03048278,  0.0069172 ,\n",
       "        -0.17132137,  0.07428142,  0.23286624,  0.02482333, -0.07760559,\n",
       "        -0.04836188, -0.05281425, -0.0042837 ,  0.04990843,  0.07437241,\n",
       "         0.15941546, -0.00058249, -0.18206593, -0.12870384, -0.00555234,\n",
       "        -0.14365066, -0.00408052, -0.0338159 , -0.06152325,  0.20513263,\n",
       "        -0.13949055, -0.03458832, -0.08145105, -0.08678653,  0.04593665,\n",
       "        -0.00506981,  0.03410221, -0.07904691,  0.03350657, -0.07918359,\n",
       "         0.10808509,  0.08982612, -0.09260518]),\n",
       " 'Elyor': array([-0.09706743, -0.05326697,  0.033822  , -0.0295127 , -0.06443704,\n",
       "        -0.11632642, -0.02904595,  0.08702158, -0.11480325,  0.09938054,\n",
       "         0.11298358, -0.02700012, -0.05656701, -0.04794815,  0.05546816,\n",
       "        -0.06526902,  0.00527467, -0.08681822, -0.00187098,  0.08511484,\n",
       "         0.0447099 , -0.04305623,  0.03312256, -0.16643516,  0.01953968,\n",
       "        -0.04253445, -0.11611269, -0.00319819, -0.11253377,  0.0603987 ,\n",
       "        -0.17841432, -0.08616254, -0.11287212,  0.00978747, -0.08133647,\n",
       "         0.08958263, -0.07832073,  0.03254563, -0.03869394, -0.05215843,\n",
       "         0.04950467, -0.03598473, -0.10870808,  0.06561312,  0.04663088,\n",
       "         0.08419541, -0.03020127, -0.05173972, -0.03164073,  0.05227193,\n",
       "        -0.05011985,  0.04289482, -0.04373357, -0.05929477, -0.01561162,\n",
       "         0.04807036, -0.02590315,  0.02402449,  0.13795265, -0.05200959,\n",
       "         0.06485123,  0.11282605, -0.03669638, -0.0428919 ,  0.03183015,\n",
       "        -0.09233904,  0.05538233,  0.01275481, -0.04116906,  0.19979815,\n",
       "        -0.111547  ,  0.02078421,  0.05805587, -0.01376753, -0.09955489,\n",
       "        -0.07062199, -0.04153023, -0.1310283 , -0.0430423 ,  0.00122088,\n",
       "         0.06954305,  0.27830716,  0.03215275, -0.07992364,  0.02734759,\n",
       "        -0.10592319, -0.01271565, -0.05480716, -0.14743418,  0.00060442,\n",
       "         0.00339445,  0.19624184,  0.09732468, -0.01410152,  0.07652911,\n",
       "        -0.18888654, -0.02169842,  0.12062568,  0.11630231, -0.02381224,\n",
       "        -0.043001  ,  0.11186172,  0.0124019 ,  0.0258429 ,  0.1618806 ,\n",
       "         0.03194166,  0.11151589, -0.11489086, -0.04954251,  0.08238186,\n",
       "        -0.02133981,  0.05236573, -0.01089867, -0.01455712,  0.05908553,\n",
       "        -0.09054131,  0.00240027,  0.0092461 , -0.03836294, -0.07111944,\n",
       "         0.08657935, -0.00396591, -0.10796176,  0.0535647 ,  0.00078499,\n",
       "         0.00224475, -0.01642712, -0.07655955]),\n",
       " 'Rustam': array([-0.04248393, -0.05681355,  0.01018845, -0.0694507 , -0.08047584,\n",
       "        -0.0627237 ,  0.04312533,  0.0622006 , -0.03789741,  0.19563987,\n",
       "         0.06241234,  0.03757516, -0.15804958, -0.05176168, -0.03392934,\n",
       "        -0.03080982, -0.06629069, -0.09873555, -0.00815197, -0.02227121,\n",
       "        -0.04675876,  0.02526823,  0.00527696, -0.16086478,  0.04239918,\n",
       "        -0.04060135, -0.04929853,  0.0198537 ,  0.00574256,  0.18425394,\n",
       "        -0.13635034, -0.14303709, -0.0240281 ,  0.16936063,  0.02045471,\n",
       "         0.08403447, -0.18729225, -0.01243703, -0.04486798, -0.04358057,\n",
       "         0.02117964,  0.05941781, -0.05304072,  0.016139  , -0.06174534,\n",
       "        -0.0556013 ,  0.07471563, -0.13255611, -0.08073461,  0.08255645,\n",
       "         0.05791433,  0.04936317, -0.12844023,  0.02777577,  0.05829445,\n",
       "        -0.01229014, -0.06218893,  0.06098809,  0.15604311, -0.05575511,\n",
       "        -0.0755556 , -0.01622385, -0.03033454, -0.06712278, -0.00042535,\n",
       "        -0.11805384,  0.07168165,  0.03732352, -0.11494832, -0.04491908,\n",
       "        -0.0705369 , -0.04676276,  0.09331433, -0.01229334,  0.00277199,\n",
       "        -0.08473079,  0.01675857, -0.21700784,  0.06089763, -0.01325544,\n",
       "         0.06797817,  0.2025427 , -0.039205  , -0.00155575, -0.00930869,\n",
       "        -0.19812603,  0.00082915,  0.03074737, -0.09288911, -0.03076894,\n",
       "         0.03816475,  0.22583354,  0.00733695, -0.0789734 , -0.11474203,\n",
       "        -0.10460616, -0.02935273,  0.083073  ,  0.09673946, -0.04323341,\n",
       "        -0.03707577,  0.03568991,  0.05823854,  0.01978187,  0.03618584,\n",
       "         0.12666195,  0.11255411, -0.17960038, -0.11279846,  0.1698765 ,\n",
       "        -0.00609969, -0.01528414,  0.02199046, -0.01262078,  0.0352117 ,\n",
       "        -0.11680667,  0.00440002, -0.05042762, -0.02905833, -0.02991649,\n",
       "         0.13208049,  0.14208513,  0.02216738,  0.02375471, -0.01474582,\n",
       "         0.02567242,  0.11892054, -0.06835893]),\n",
       " 'Sardor': array([-0.21063937, -0.04949095,  0.07417961, -0.07266234, -0.05894153,\n",
       "        -0.10701992,  0.02074288, -0.04346952, -0.13598647,  0.06964455,\n",
       "         0.06533645, -0.04566201,  0.04155387,  0.01138106,  0.03839875,\n",
       "        -0.00349417, -0.00866196, -0.01611721,  0.04533788,  0.06056676,\n",
       "         0.00363508, -0.10599342,  0.06878502, -0.14748001, -0.02193576,\n",
       "         0.00072484, -0.10931668,  0.10711513, -0.20692167,  0.00367634,\n",
       "        -0.13684987, -0.04624716, -0.10456062,  0.0444714 , -0.16017935,\n",
       "         0.08281756, -0.07952589,  0.14259914, -0.0474221 , -0.07510702,\n",
       "         0.04629162, -0.01448937, -0.01254605,  0.07514943,  0.0100617 ,\n",
       "         0.00639459, -0.07421954, -0.05967369,  0.00750822,  0.08921888,\n",
       "        -0.03570121, -0.07184995,  0.02997596, -0.04081291,  0.01733415,\n",
       "         0.05004876,  0.03174153,  0.12676772,  0.12174352, -0.02869141,\n",
       "         0.06353831,  0.12744718, -0.00869549, -0.00833152, -0.05005367,\n",
       "        -0.04442418, -0.0019116 ,  0.00040764, -0.06615139,  0.18601514,\n",
       "        -0.10830252,  0.04659425,  0.04088518, -0.01067553, -0.12221157,\n",
       "        -0.04569051, -0.05388561, -0.03268217, -0.0846619 ,  0.07568448,\n",
       "         0.11240503,  0.19432531,  0.07816011,  0.02878203, -0.07506156,\n",
       "        -0.11368937,  0.09125875, -0.06057031, -0.13039144,  0.06159898,\n",
       "         0.01802175,  0.17638326,  0.01931103, -0.02532958, -0.00818731,\n",
       "        -0.10038417,  0.02135598,  0.14214148,  0.10607242, -0.00874325,\n",
       "        -0.03545748,  0.13632401,  0.07334446,  0.01587977,  0.149881  ,\n",
       "         0.14173757,  0.12038026, -0.0112095 , -0.09848896,  0.05123783,\n",
       "        -0.06690171,  0.10992423,  0.01994179, -0.02009101,  0.04967254,\n",
       "        -0.12739061,  0.07332668,  0.05808969,  0.01747072, -0.01326029,\n",
       "         0.08217003,  0.02635927, -0.1291935 ,  0.10531249,  0.01172082,\n",
       "         0.05733471, -0.01501659, -0.08221166])}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pipresence.config import Config\n",
    "from pipresence.tools.utils import load_database\n",
    "\n",
    "\n",
    "Config.update_config(\n",
    "    embeddings_file = \"../data/encodings/face_embeddings.pkl\"\n",
    ")\n",
    "database = load_database()\n",
    "\n",
    "database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
